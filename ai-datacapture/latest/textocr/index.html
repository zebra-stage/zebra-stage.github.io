
<!DOCTYPE html>
<html lang="en"><head>

    <meta charset="utf-8">
    <meta http-equiv="Pragma" content="no-cache">
    <meta http-equiv="Expires" content="-1">
    <meta http-equiv="CACHE-CONTROL" content="NO-CACHE">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description" content="Online Documentation for Zebra Technologies developer tools and utilties, including EMDK for Android, EMDK for Xamarin, StageNow and Enterprise Browser.">
    <meta name="author" content="Zebra Technologies">
    <meta name="google-site-verification" content="i4B78BrWnNy8ShJwe5feRW3jO3HE6gfYlYqYbxfl8yY">
    <meta name="msvalidate.01" content="6B651B00161BCE79B8950AC09D5C4C75">
    <meta title="Text OCR">
    <title>Text OCR - TechDocs</title>
    <!-- core CSS -->
    <link href="/css/bootstrap.min.css" rel="stylesheet">
<!--     
    <link href="/css/font-awesome.min.css" rel="stylesheet">
 -->    
    <link href="/css/all.css" rel="stylesheet">
    <link href="/css/v4-shims.css" rel="stylesheet">
    <link href="/css/animate.min.css" rel="stylesheet">
    <link href="/css/owl.carousel.css" rel="stylesheet">
    <link href="/css/owl.transitions.css" rel="stylesheet">
    <link href="/css/prettyPhoto.css" rel="stylesheet">
    <link href="/css/main.css" rel="stylesheet">
    <link href="/css/responsive.css" rel="stylesheet">
    <link href="/css/prettify.css" rel="stylesheet">
    <link href="/css/bootstrap-treenav.min.css" rel="stylesheet">
    <!--[if lt IE 9]>
    <script src="js/html5shiv.js"></script>
    <script src="js/respond.min.js"></script>
    <![endif]--> 

    <!-- 6/13/24- a "crown" icon randomly appeared in the MX matrix today. Rob said it looked like sumo. 
        <script src="//load.sumome.com/" data-sumo-site-id="699cb66cf4dc59352efb45705526d15cbe314e1cd43f7761b94d96f3cf7338e8" async="async"></script> -->
    <link rel="shortcut icon" href="/favicon.ico">
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-01QJCMQM9N"></script><script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());
  gtag('config', 'G-01QJCMQM9N');
</script><script src="/js/jquery.js"></script></head><!--/head-->

<!-- Google tag -->





<body id="home" class="homepage" data-spy="scroll" data-offset="100" data-target="#toc">

    <header id="header">
        <nav id="main-menu" class="navbar navbar-default navbar-fixed-top" role="banner">
            <div class="container-fluid">
                <div class="navbar-header">
                    <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-collapse">
                        <span class="sr-only">Toggle navigation</span>
                        <span class="icon-bar"></span>
                        <span class="icon-bar"></span>
                        <span class="icon-bar"></span>
                    </button>
                    <a class="navbar-brand" href="/"><img src="/images/logo.png" alt="logo" style="max-height:66px;float: inherit;
    padding-right: 3px;"></a> 
                    
                   
                </div>
                <div class="collapse navbar-collapse navbar-left">
                    <ul class="nav navbar-nav">
                        <li class="navbar-text pull-left"><strong>TechDocs</strong></li>
                        <p class="navbar-text">
                                            AI Data Capture SDK
                        </p>

                    </ul>
                </div>
                <div class="collapse navbar-collapse navbar-right">
                    <ul class="nav navbar-nav">
                        


                        <li>
                            <a href="/ai-datacapture/latest/about">
                                    About
                            </a>
                        </li>
                        <li>
                            <a href="/ai-datacapture/latest/setup">
                                    Setup
                            </a>
                        </li>
                        <li>
                            <a href="/ai-datacapture/latest/localizer">
                                    Localizer
                            </a>
                        </li>
                        <li>
                            <a href="/ai-datacapture/latest/productrecognition">
                                    Product Recognition
                            </a>
                        </li>
                        <li>
                            <a href="/ai-datacapture/latest/barcodedecoder">
                                    Barcode Decoder
                            </a>
                        </li>
                        <li>
                            <a href="/ai-datacapture/latest/textocr">
                                    Text OCR
                            </a>
                        </li>
                        <li>
                            <a href="/ai-datacapture/latest/camerax">
                                    CameraX
                            </a>
                        </li>
                        <li>
                            <a href="/ai-datacapture/latest/search">
                                    <i class="fa fa-search"></i>
                            </a>
                        </li>
                        
                        <!-- <li ><a href="/products">SDKs</a></li>
                        <li ><a href="/samples">Samples</a></li>
                        <li ><a href="/guides">Guides</a></li>
                        <li ><a href="/tutorials">Tutorials</a></li>
                        <li ><a href="/apis">APIs</a></li> -->
                        <!-- <li><a href="#"><input type="text" class="st-default-search-input"></a></li> -->
                    </ul>
                </div>
            </div><!--/.container-->
        </nav><!--/nav-->
    </header><!--/header-->
<div class="container-fluid">
    <section id="blog">
        <div class="">
            <section class="content-with-sidebar">
            <div class="row">
                <div class="col-sm-3 hidden-sm hidden-xs" id="sidebar">
                    
                    
                    <nav id="toc" data-toggle="toc" data-spy="affix" style="height: 75%; max-width: 225px; overflow-y: auto; ">
                        
                        <ul class="nav">
                            <li><strong>Text OCR</strong></li>
                        </ul>
                    </nav>
                    <div class="modal fade" id="modal-smartdocs" tabindex="-1" role="dialog" aria-labelledby="basicModal" aria-hidden="true" data-backdrop="true">
                    	<div class="modal-dialog">
                    		<div class="modal-content">
                    			<div class="modal-header">
                    				<button type="button" class="close" data-dismiss="modal" aria-hidden="true">X</button>
                    			</div>
                    			<div class="modal-body">
                    				<form>
                    				  <div class="form-group">
                    				    <label for="exampleInputEmail1">MX Version On Device:</label>
                    				    <select class="form-control">
                    					  <option value="4.0">4.0</option>
                    					  <option value="4.1">4.1</option>
                    					  <option value="4.2">4.2</option>
                    					  <option value="4.3">4.3</option>
                    					  <option value="5.0">5.0</option>
                    					</select>
                    				  </div>						
                    				  <div class="form-group">
                    				    <label for="exampleInputEmail1">OSX Version On Device:</label>
                    				    <select class="form-control">
                    					  <option value="1.0">1.0</option>
                    					  <option value="1.2">1.2</option>
                    					  <option value="1.3.4-56">1.3.4-56</option>
                    					  <option value="3.4">3.4</option>
                    					  <option value="3.5">3.5</option>
                    					  <option value="3.6">3.6</option>
                    					  <option value="4.1">4.1</option>
                    					  <option value="4.2">4.2</option>
                    					  <option value="4.3">4.3</option>
                    					  <option value="4.4">4.4</option>
                    					</select>
                    				  </div>						
                    				  <div class="form-group">
                    				    <label for="exampleInputEmail1">Android Version On Device:</label>
                    				    <select class="form-control">
                    					  <option value="23">6.0 Marshmallow</option>
                    					  <option value="22">5.1 Lollipop</option>
                    					  <option value="21">5.0 Lollipop</option>
                    					  <option value="19">4.4 Kitkat</option>
                    					  <option value="18">4.3 Jellybean</option>
                    					  <option value="17">4.2.x Jellybean</option>
                    					  <option value="16">4.1.x Jellybean</option>
                    					  <option value="15">4.0.3-4.0.4 Ice Cream Sandwich</option>
                    					  <option value="14">4.0-4.0.2 Ice Cream Sandwich</option>
                    					  <option value="10">2.3.3-2.3.4 Gingerbread</option>
                    					</select>
                    				  </div>
                    				  <div class="form-group">
                    				  	<button class="primary" id="btn_SaveSmartDocOptions">Save Changes</button><button class="primary" id="btn_ClearSmartDocOptions">Clear Changes</button>
                    				  </div>
                    				</form>
                    			</div>
                    	</div>
                      </div>
                    </div>
                    
                    <script type="text/javascript">
                        $(document).ready(function() {
                        	$( "#btn_SaveSmartDocOptions" ).click(function() {
                      			console.log('Saving Options');
                      			$("#modal-smartdocs").modal({show: false});
                    		});
                        	$( "#btn_SaveSmartDocOptions" ).click(function() {
                      			console.log('Clearing Options');
                      			$("#modal-smartdocs").modal({show: false});
                    		});
                        });
                    </script><link rel="stylesheet" href="https://cdn.rawgit.com/afeld/bootstrap-toc/v0.3.0/dist/bootstrap-toc.min.css">
                    <script src="https://cdn.rawgit.com/afeld/bootstrap-toc/v0.3.0/dist/bootstrap-toc.min.js"></script>
                    <script type="text/javascript">
                        $(document).ready(function() {
                    
                        });
                    
                    
                    </script>
                </div>                
                <div class="col-sm-9 ">
                    <div class="row">
                        <div class="col-sm-12">
                            <section id="cta" class="wow fadeIn">
                                <div class="">
                                    <h1 id="text-ocr" class="anchor"><a class="heading-anchor" href="#text-ocr"><span></span></a>Text OCR</h1>
                                    <p>
                                            AI Data Capture SDK
                                    </p>
                                </div>
                            </section>                    
                        </div>
                    </div>
                    <div id="mainContent" class="row">
                        <div class="col-sm-12">
                            <div class="blog-post blog-large wow fadeInLeft" data-wow-duration="300ms" data-wow-delay="0ms">
                            <!-- Start of guide -->
                                <div class="service-box wow fadeInRight">
                                    <div class="">

                                        <p></p><h2 id="overview" class="anchor"><a class="heading-anchor" href="#overview"><span></span></a>Overview</h2>

<p>The <code>TextOCR</code> class leverages Optical Character Recognition (OCR) to detect, recognize, and group text from images. The Text OCR process consists of three stages: </p>

<ol>
<li><strong><a href="#textdetecdtion">Text Detection</a> -</strong> Identifies and filters text boxes within the image.</li>

<li><strong><a href="#textrecognition">Text Recognition</a> -</strong> Reads and extracts text content from each identified text box. </li>

<li><strong><a href="#textgrouping">Text Grouping</a> -</strong> Organizes recognized words into lines or paragraphs. </li>
</ol>

<p>This guide begins with initial instructions for basic setup of OCR. Subsequent sections offer a comprehensive list of OCR settings that can be fine-tuned for specific needs:</p>

<ul>
<li><a href="../class/inferenceroptions/">Inferencer Options</a> </li>

<li><a href="#detectionparameters">Detection Parameters</a></li>

<li><a href="#recognitionparameters">Recognition Parameters</a>


<ul>
<li><a href="#tilingsettings">Tiling Settings</a></li></ul>
</li>

<li><a href="#groupersettings">Grouper Settings</a></li>
</ul>

<p>For non-standard use cases, particularly for <a href="#groupersettings">Grouper Settings</a>, developers are encouraged to experiment with and adjust these parameters to optimize performance.</p>

<p>Additionally, the process() method of the <code>TextOCR</code> class can be utilized to detect and recognize text within images. This interface enables developers to build CameraX analyzers that integrate with other detectors.</p>

<hr>

<h2 id="aimodel" class="anchor"><a class="heading-anchor" href="#aimodel"><span></span></a>AI Model</h2>

<p>The <strong>text-ocr-recognizer</strong> model is designed for text recognition tasks. For more information and to download the model, refer to <a href="../model/textocr">Text OCR Model</a>.</p>

<hr>

<h2 id="capabilities" class="anchor"><a class="heading-anchor" href="#capabilities"><span></span></a>Capabilities</h2>

<h3 id="supportedcharacters" class="anchor"><a class="heading-anchor" href="#supportedcharacters"><span></span></a>Supported Characters</h3>

<p>TextOCR recognizes a range of characters, including:</p>

<pre class="prettyprint"><code>    *0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ!\"#$%&amp;'()*+,-./:;&lt;=&gt;?@[\\]^_`{|}~*
</code></pre>

<p>By default, it supports a maximum word length of approximately 15 characters, though this limit may decrease with the use of uncommon fonts. Enabling <a href="#tilingsettings">tiling</a> removes this restriction.</p>

<h3 id="inputoutput" class="anchor"><a class="heading-anchor" href="#inputoutput"><span></span></a>Input/Output</h3>

<p><strong>Input Parameters:</strong> The default model input size is 640x640 pixels, but this can be adjusted during runtime initialization.</p>

<p><strong>Output Parameters:</strong> The output consists of a list of text detections, each accompanied by a list of <a href="../types/#complexbbox">complex bounding boxes</a> that define the location and content of the detected text.</p>

<hr>

<h2 id="configuration" class="anchor"><a class="heading-anchor" href="#configuration"><span></span></a>Configuration</h2>

<p>Before starting with TextOCR, configure key settings such as model input size, resolution, and inference type. Changes to these settings will require reinitializing the models. Information on configuring these settings are provided in the sections that follow.</p>

<h3 id="modelinputsize" class="anchor"><a class="heading-anchor" href="#modelinputsize"><span></span></a>Model Input Size</h3>

<p>The Model Input Size defines the resolution at which the AI processes images. Before analysis, images are resized to this dimension. Adjusting this size balances speed and accuracy.</p>

<p><strong>Key Considerations:</strong></p>

<ul>
<li>Start with the default resolution 640x640 for optimal processing.</li>

<li>If the results are not sufficiently accurate for small text, increasing the resolution can improve precision.</li>
</ul>

<table class="facelift" style="width:20" border="1" padding="5px">  
  <thead>
    <tr bgcolor="#dce8ef">
      <th>Input Size</th>
      <th>Best For</th>
      <th>Use Case</th>
      <th>Consideration</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Smaller (e.g., 640x640)</td>
      <td>Speed: Faster processing</td>
      <td>Large or close text</td>
      <td>Reduced accuracy for small text</td>
    </tr>
    <tr>
      <td>Larger (e.g., 1600x1600)</td>
      <td>Accuracy: Better for details</td>
      <td>Fine print, distant, or dense text</td>
      <td>Slower processing, higher memory usage</td>
    </tr>
    <tr>
      <td>Custom (Multiples of 32, e.g., 800x800)</td>
      <td>Balancing speed and accuracy</td>
      <td>Low-contrast or medium-sized text</td>
      <td>Requires experimentation to find the optimal setting</td>
    </tr>
  </tbody>
</table>

<p><br></p>

<h3 id="resolution" class="anchor"><a class="heading-anchor" href="#resolution"><span></span></a>Resolution</h3>

<p>Camera resolution refers to the number of pixels the device’s camera sensor can capture (e.g., 1MP = 1280x720, 8MP = 3840x2160). It determines the quality of the source image before any resizing occurs for AI processing.</p>

<p>Higher camera resolution provides a more detailed and higher-quality original image, which can significantly enhance the AI model's ability to detect and recognize small, faint, or distant text. However, this increased detail comes at the cost of greater processing power and memory usage.</p>

<p><strong>Key Considerations:</strong></p>

<ul>
<li><strong>Impact of Camera Resolution -</strong> Higher resolutions enhance input image detail, aiding in recognizing small, low-contrast, or distant text. However, images are downscaled to the model's input size for processing, so the benefits of high-resolution cameras diminish with low model input sizes.</li>

<li><strong>General Guidance -</strong> Aim for a minimum text height of 16 pixels in the input image, adjusting for font size and camera distance from the target.</li>
</ul>

<table class="facelift" style="width:20" border="1" padding="5px">  
  <thead>
    <tr bgcolor="#dce8ef">
      <th>Resolution</th>
      <th>Best For</th>
      <th>Use Case</th>
      <th>Consideration</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>1MP (1280x720)</td>
      <td>Speed, power efficiency</td>
      <td>Large/simple text</td>
      <td>May miss fine details</td>
    </tr>
    <tr>
      <td>2MP (1920x1080)</td>
      <td>General use</td>
      <td>Stylized or moderately detailed text</td>
      <td>Balanced performance</td>
    </tr>
    <tr>
      <td>4MP (2688x1512)</td>
      <td>Detailed scans</td>
      <td>Contracts, forms, dense text</td>
      <td>Higher memory and battery use</td>
    </tr>
    <tr>
      <td>8MP (3840x2160)</td>
      <td>Maximum detail</td>
      <td>Archival purposes</td>
      <td>Large files, diminishing returns for low input sizes</td>
    </tr>
  </tbody>
</table>

<p><br></p>

<p><strong>Relationship Between Model Input Size and Camera Resolution:</strong></p>

<table class="facelift" style="width:20" border="1" padding="5px">  
  <thead>
    <tr bgcolor="#dce8ef">
      <th>Resolution</th>
      <th>Low Input Size (e.g., 640x640)</th>
      <th>High Input Size (e.g., 1600x1600)</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><strong>Low Resolution (e.g., 1MP / 1280x720)</strong></td>
      <td>
        <strong>Speed:</strong> Fastest<br>
        <strong>Accuracy:</strong> Lowest<br>
        <strong>Use Case:</strong> Large, clear text close to the camera<br>
        <strong>Note:</strong> Small/fine details may be lost
      </td>
      <td>
        <strong>Not Recommended:</strong> Wasted computation with little accuracy gain
      </td>
    </tr>
    <tr>
      <td><strong>High Resolution (e.g., 8MP / 3840x2160)</strong></td>
      <td>
        <strong>Speed:</strong> Fast<br>
        <strong>Accuracy:</strong> Moderate<br>
        <strong>Use Case:</strong> Large/medium text, quick scans<br>
        <strong>Note:</strong> High-resolution source is downsampled for AI, so small text may still be missed
      </td>
      <td>
        <strong>Speed:</strong> Slowest<br>
        <strong>Accuracy:</strong> Highest<br>
        <strong>Use Case:</strong> Detailed, small, dense, or distant text<br>
        <strong>Note:</strong> High memory and battery usage; may stress low-end devices
      </td>
    </tr>
  </tbody>
</table>

<p><br></p>

<h3 id="inferencertypeprocessor" class="anchor"><a class="heading-anchor" href="#inferencertypeprocessor"><span></span></a>Inferencer Type (Processor)</h3>

<p>The <strong>Inferencer Type</strong> specifies which chip on the device is responsible for performing AI computations (referred to as "inference"). This choice directly impacts the speed and efficiency of image processing.</p>

<p><strong>Key Considerations:</strong></p>

<ul>
<li><strong>DSP (Digital Signal Processor) -</strong> Use DSP if available, as it is specifically designed for real-time, energy-efficient AI tasks and provides optimal performance.</li>

<li><strong>GPU (Graphics Processing Unit) -</strong> If DSP is not available, the GPU serves as an alternative for handling AI workloads efficiently.</li>
</ul>

<table class="facelift" style="width:20" border="1" padding="5px">  
  <thead>
    <tr bgcolor="#dce8ef">
      <th>Processor</th>
      <th>Description</th>
      <th>Performance</th>
      <th>Use Case</th>
      <th>Device Platform</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><strong>DSP (Digital Signal Processor)</strong></td>
      <td>
        Optimized for real-time, energy-efficient tasks. Ideal for specific AI workloads where battery life and efficiency are critical.
      </td>
      <td>
        <ul>
          <li><strong>Best:</strong> Fastest and most efficient.</li>
          <li>Preserves battery life during continuous use.</li>
        </ul>
      </td>
      <td>
        Best choice for real-time, low-energy tasks such as edge AI inference.
      </td>
      <td><strong>Best for:</strong> SD6490, SD5430 FP2; for relevant device models, visit <a href="https://supportcommunity.zebra.com/s/article/000022440?language=en_US">Zebra Platform Devices</a></td>
    </tr>
    <tr>
      <td><strong>GPU (Graphics Processing Unit)</strong></td>
      <td>
        Designed for heavy, parallel AI tasks and complex models. Suitable for computationally intensive workloads.
      </td>
      <td>
        <ul>
          <li><strong>Good:</strong> High-speed performance but consumes more power than DSP.</li>
          <li>Handles large-scale parallel tasks effectively.</li>
        </ul>
      </td>
      <td>
        Best for handling complex AI models or tasks requiring significant computational power.
      </td>
      <td><strong>Best for:</strong> SD4490, SD5430 FP1<br><strong>Good for:</strong> SD6490, SD5430 FP2; for relevant device models, visit <a href="https://supportcommunity.zebra.com/s/article/000022440?language=en_US">Zebra Platform Devices</a></td>
    </tr>
    <tr>
      <td><strong>CPU (Central Processing Unit)</strong></td>
      <td>
        Acts as the fallback processor for AI inference tasks. Always available but less efficient compared to DSP and GPU.
      </td>
      <td>
        <ul>
          <li><strong>Fallback:</strong> Slower and less efficient but always available.</li>
          <li>Consumes more power, making it less suitable for continuous tasks.</li>
        </ul>
      </td>
      <td>
        Suitable for lightweight tasks or as a fallback when DSP or GPU are unavailable or causing issues.
      </td>
      <td><strong>Fallback for:</strong> SD6490, SD4490, SD5430 FP2, SD5430 FP1; for relevant device models, visit <a href="https://supportcommunity.zebra.com/s/article/000022440?language=en_US">Zebra Platform Devices</a></td>
    </tr>
  </tbody>
</table>

<!-- 
---

## Stages of TextOCR

The **Text OCR** process consists of three stages:

1. **Text Detection -** Identifies and filters text boxes within the image
2. **Text Recognition -** Reads and extracts text content from each identified text box.
3. **Text Grouping -** Organizes recognized words into lines or paragraphs.

Information about each stage is provided in the subsequent sections.

### Stage 1: Text Detection

The text detection process occurs in two main stages:

1. **Heatmap Threshold (Pixel-Level Filtering) -** A heatmap is generated where each pixel is assigned a score indicating the likelihood of it being part of a text character. The **Heatmap Threshold** filters out individual pixels based on these scores, retaining only the most probable candidates.
2. **Box Threshold (Box-Level Filtering) -** After pixel filtering, the system identifies groups of pixels and draws bounding boxes around them. Each box is assigned a confidence score, and the **Box Threshold** filters out boxes with low confidence, retaining only those likely to contain text.

#### Filtering Parameters

Once potential text boxes are identified, additional filtering can be applied to refine results by eliminating noise or unwanted detections. These filters also allow adjustments to the image and bounding box handling:

- **Min Box Size (Filtering Narrow Boxes) -** Removes text boxes that are too narrow or short, such as underscores, divider lines, or other elongated shapes that are unlikely to contain text.
- **Min Box Area (Filtering Small Boxes) -** Excludes text boxes with a total area (width × height) that is too small. This is particularly effective for ignoring noise such as tiny specks or dots in the image.
- **Unclip Ratio (Expanding Box Size) -** Expands the bounding boxes after they are detected but before text recognition occurs. Initial bounding boxes are often tightly fitted to the text, and this parameter increases their size to include some background, improving text recognition accuracy.
- **Min Ratio for Rotation (Handling Vertical Text) -** Rotates vertically oriented boxes (e.g., bottom-up or top-down text) to a horizontal orientation for better recognition. Applies only to boxes with a height-to-width ratio exceeding the specified value, rotating them 90 degrees to facilitate decoding.


<table class="facelift" style="width:20" border="1" padding="5px">  
  <thead>
    <tr bgcolor="#dce8ef">
      <th>Parameter</th>
      <th>Description</th>
      <th>Guidance</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><strong>Heatmap Threshold</strong></td>
      <td>
        Minimum pixel confidence for potential text regions. (Pixel-level filtering)
      </td>
      <td>
        <ul>
          <li><strong>↑</strong> To cut noise, best for clear text or high contrast.</li>
          <li><strong>↓</strong> To capture faint, curved, or blurred text with low contrast.</li>
        </ul>
      </td>
    </tr>
    <tr>
      <td><strong>Box Threshold</strong></td>
      <td>
        Filters detected boxes by their overall confidence. (Box-level filtering)
      </td>
      <td>
        <ul>
          <li><strong>↑</strong> To reduce false positives.</li>
          <li><strong>↓</strong> To catch weak detections.</li>
        </ul>
      </td>
    </tr>
    <tr>
      <td><strong>Min Box Size</strong></td>
      <td>
        Filters out text boxes that are too narrow or too short. (Filtering "skinny" boxes)
      </td>
      <td>
        <ul>
          <li><strong>↑</strong> To ignore divider lines, underscores, or non-text lines.</li>
          <li><strong>↓</strong> To detect smaller text.</li>
        </ul>
      </td>
    </tr>
    <tr>
      <td><strong>Min Box Area</strong></td>
      <td>
        Filters out text boxes if their total area (width × height) is too small. (Filtering "tiny" boxes)
      </td>
      <td>
        <ul>
          <li><strong>↑</strong> To eliminate dust, dots, or tiny artifacts.</li>
          <li><strong>↓</strong> To allow smaller text or marks.</li>
        </ul>
      </td>
    </tr>
    <tr>
      <td><strong>Unclip Ratio</strong></td>
      <td>
        Expands or "stretches" detected boxes outward to include full characters and some background. (Expands box size)
      </td>
      <td>
        <ul>
          <li><strong>↑</strong> For curved, rotated, or incomplete detections.</li>
          <li><strong>↓</strong> To avoid overlapping with neighboring text regions or noisy regions.</li>
        </ul>
      </td>
    </tr>
    <tr>
      <td><strong>Min Ratio for Rotation</strong></td>
      <td>
        Rotates vertically (high height, low width) oriented boxes so they become horizontal. (Rotates vertical boxes)
      </td>
      <td>
        Only adjust if the images contain many rotated texts (e.g., upright or vertical text).
      </td>
    </tr>
  </tbody>
</table>


**Legend:**

- **↑:** Increase the value of the parameter.
- **↓:** Decrease the value of the parameter.

### Stage 2: Text Recognition

After text boxes are detected, the next step is to extract and accurately read the text within each bounding box.
AI Suite uses the **"Total" decoder** to convert character predictions into meaningful words, even in cases where the model is uncertain about specific characters.

The **"Total" decoder** evaluates all possible character options for each position and balances confidence and flexibility using two key parameters: **TopK Ignore Cutoff, Total Prob Threshold,** and **Max Word Combinations.** These parameters act as filters to refine predictions and determine the final output.

#### Decoder Parameters


<table class="facelift" style="width:20" border="1" padding="5px">  
  <thead>
    <tr bgcolor="#dce8ef">
      <th>Parameter</th>
      <th>Description</th>
      <th>Guidance</th>
      <th>Example</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><strong>TopK Ignore Cutoff</strong> <br> <i>The Gatekeeper</i></td>
      <td>
        The maximum number of highest-confidence character predictions the "Total" decoder considers for each character position. If additional are needed to meet the <b>Total Prob Threshold,</b> the model outputs a replacement character (e.g., "�").
      </td>
      <td>
        <ul>
          <li><strong>↑</strong> If the desired character isn’t appearing, allow more guesses.</li>
          <li>Leave at default for most cases; adjust only if necessary.</li>
        </ul>
      </td>
      <td>
        <strong>Example:</strong>  
        If predictions are: 'S' (40%), 's' (30%), '5' (15%), 'B' (5%), '8' (2%) And the cutoff is set to 4, only the top four predictions ('S', 's', '5', 'B') are retained. '8' and lower-ranked options are discarded.
      </td>
    </tr>
    <tr>
      <td><strong>Total Prob Threshold</strong> <br> <i>The Quality Check</i></td>
      <td>
        The minimum cumulative confidence score required for a word prediction to be accepted. If the total score is below the threshold, the model outputs a replacement character (e.g., "�").
      </td>
      <td>
        <ul>
          <li><strong>↑</strong> For more flexible but potentially noisier results.</li>
          <li><strong>↓</strong> For more trustworthy, reliable outputs.</li>
        </ul>
      </td>
      <td>
        <strong>Example:</strong> Using the predictions above, their combined confidence is:  
        0.40 (S) + 0.30 (s) + 0.15 (5) + 0.05 (B) = 0.90. If the threshold is 0.85, the decoder proceeds. If the combined score is less, it outputs a replacement character (e.g., "�").
      </td>
    </tr>
    <tr>
      <td><strong>Max Word Combinations</strong> <br> <i>Result Limiter</i></td>
      <td>
        Restricts the number of valid word outputs generated from possible character combinations for each detection. This helps avoid overwhelming results, particularly for ambiguous inputs, by limiting the model’s consideration of all potential character combinations across all positions in the word.
      </td>
      <td>
        <ul>
          <li><strong>↑</strong> For tougher, longer words to see more possible outputs.</li>
          <li><strong>↓</strong> For faster processing and fewer alternatives.</li>
        </ul>
      </td>
      <td>
        <strong>Example:</strong> Suppose there are 20 possible valid word combinations after applying the above filters. If Max Word Combinations is set to 5, only the top 5 most confident word outputs are returned. The other 15 combinations are ignored, even if valid.
      </td>
    </tr>
  </tbody>
</table>


**Legend:**

- **↑:** Increase the value of the parameter.
- **↓:** Decrease the value of the parameter.

#### Recognition: Special Cases

These features are designed for specific scenarios and are generally not required for most OCR tasks. Use them only when needed to address unique text recognition challenges.

##### Flip

Runs recognition in multiple orientations to improve accuracy for rotated or flipped text. While this increases accuracy for text in varying orientations, it also adds processing time.


<table class="facelift" style="width:20" border="1" padding="5px">  
  <thead>
    <tr bgcolor="#dce8ef">
      <th>What It Does</th>
      <th>When/How to Adjust It</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>
        Runs recognition in multiple orientations to handle rotated or flipped text.
      </td>
      <td>
        <ul>
          <li><strong>Enable:</strong> If text orientation varies significantly.</li>
          <li><strong>Disable:</strong> For standard horizontal text to reduce processing time.</li>
        </ul>
      </td>
    </tr>
  </tbody>
</table>


##### Tiling

Splits very long, thin lines of text (e.g., serial numbers, document titles, part numbers) into smaller, manageable "tiles" for better recognition. This is useful when a word box exceeds the recognition limit (15 characters). While tiling improves accuracy, it also adds processing time, so it should be used only when necessary.


<table class="facelift" style="width:20" border="1" padding="5px">  
  <thead>
    <tr bgcolor="#dce8ef">
      <th>Parameter</th>
      <th>What It Does</th>
      <th>When/How to Adjust It</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><strong>Aspect Ratio Lower Threshold</strong></td>
      <td>
        Sets the minimum width-to-height ratio for boxes to be tiled. Only boxes wider than this value are tiled, helping to control which boxes are considered "elongated."
      </td>
      <td>
        <ul>
          <li><strong>↑</strong> To tile only very long boxes.</li>
          <li><strong>↓</strong> To tile shorter boxes, resulting in more tiled boxes.</li>
        </ul>
      </td>
    </tr>
    <tr>
      <td><strong>Aspect Ratio Upper Threshold</strong></td>
      <td>
        Sets the maximum width-to-height ratio for boxes to be tiled. Only boxes up to this ratio get tiled. This prevents extremely long or oddly shaped boxes from being tiled.
      </td>
      <td>
        <ul>
          <li><strong>↑</strong> To allow longer boxes to be tiled.</li>
          <li><strong>↓</strong> To avoid tiling very stretched or oddly shaped boxes.</li>
        </ul>
      </td>
    </tr>
    <tr>
      <td><strong>TopK Merged Predictions</strong></td>
      <td>
        Limits the number of "best guess" decodes returned after tiling.
      </td>
      <td>
        <ul>
          <li><strong>↑</strong> To see more possible results for review.</li>
          <li><strong>↓</strong> For fewer, faster results.</li>
        </ul>
      </td>
    </tr>
  </tbody>
</table>


**Legend:**

- **↑:** Increase the value of the parameter.
- **↓:** Decrease the value of the parameter.

### Stage 3: Text Grouping

**Text grouping** is used to organize OCR results into logical chunks, such as lines and paragraphs, for better structure and readability. The grouping process relies on several parameters that define spacing, alignment, and font size differences.


<table class="facelift" style="width:20" border="1" padding="5px">  
  <thead>
    <tr bgcolor="#dce8ef">
      <th>Parameter</th>
      <th>What It Does</th>
      <th>When/How to Adjust It</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><strong>Width Distance Ratio</strong></td>
      <td>
        Defines how much space is allowed between words before they are considered part of separate lines.
      </td>
      <td>
        <ul>
          <li><strong>↑</strong> If wide spaces between words should still be grouped in the same line.</li>
          <li><strong>↓</strong> If words are being incorrectly joined into the same line.</li>
        </ul>
      </td>
    </tr>
    <tr>
      <td><strong>Height Distance Ratio</strong></td>
      <td>
        Allows grouping of words into a single line even if their font sizes are significantly different.
      </td>
      <td>
        <ul>
          <li><strong>↑</strong> If words with different font sizes should be grouped together in a line.</li>
          <li><strong>↓</strong> If large font-size differences are causing messy groupings.</li>
        </ul>
      </td>
    </tr>
    <tr>
      <td><strong>Center Distance Ratio</strong></td>
      <td>
        Enables grouping of words into a single line even if they are not perfectly aligned (e.g., curved or wavy text).
      </td>
      <td>
        <ul>
          <li><strong>↑</strong> If curved or zigzag text should be grouped in the same line.</li>
          <li><strong>↓</strong> If only straight lines should be grouped together.</li>
        </ul>
      </td>
    </tr>
    <tr>
      <td><strong>Paragraph Height Distance</strong></td>
      <td>
        Specifies the maximum distance between lines that can still be grouped into the same paragraph.
      </td>
      <td>
        <ul>
          <li><strong>↑</strong> If lines spaced far apart should still be grouped into one paragraph.</li>
          <li><strong>↓</strong> If too many lines are being grouped into a single paragraph.</li>
        </ul>
      </td>
    </tr>
    <tr>
      <td><strong>Paragraph Height Ratio Threshold</strong></td>
      <td>
        Allows lines with very different heights (font sizes) to be grouped into the same paragraph.
      </td>
      <td>
        <ul>
          <li><strong>↑</strong> If lines with similar font sizes should be grouped.</li>
          <li><strong>↓</strong> To allow lines with very different font sizes to be grouped together.</li>
        </ul>
      </td>
    </tr>
  </tbody>
</table>


**Legend:**

- **↑:** Increase the value of the parameter.
- **↓:** Decrease the value of the parameter.

-->

<hr>

<h2 id="developerguide" class="anchor"><a class="heading-anchor" href="#developerguide"><span></span></a>Developer Guide</h2>

<p>This guide outlines the process for using TextOCR to detect and recognize text within images, from initialization to outputting the identified text.</p>

<h3 id="step1initialization" class="anchor"><a class="heading-anchor" href="#step1initialization"><span></span></a>Step 1: Initialization</h3>

<p>Follow these steps to set up and initialize a <code>TextOCR</code> object:</p>

<ol>
<li><p><strong>Import the TextOCR class:</strong> Use <code>com.zebra.ai.vision.detector.TextOCR</code>.</p></li>

<li><p><strong>Initialize the SDK:</strong> Use your application's context object and invoke <code>init()</code> from the <a href="../class/aivisionsdk/">AIVisionSDK</a> class.</p></li>

<li><p><strong>Configure OCR Settings:</strong> Create a <code>TextOCR.Settings</code> object.</p></li>

<li><p><strong>Optional: Set model input dimensions:</strong> If needed, customize the model input dimensions (height and width). These should be multiples of 32 (e.g., 640). For guidance, see <a href="#modelinputsize">Model Input Size</a>.</p>

<pre class="prettyprint"><code>settings.detectionInferencerOptions.defaultDims.width = [your value];
settings.detectionInferencerOptions.defaultDims.height = [your value];
</code></pre>

<ul>
<li><strong>Smaller Input Sizes -</strong> Reduce processing time and increase speed, but may decrease accuracy. Ideal for larger or closer text.</li>

<li><strong>Larger Input Sizes -</strong> Improve accuracy for smaller or more distant text, but increase inference time. An input size that is too large may cause out-of-memory errors and potentially cause an application crash at run-time.</li></ul></li>

<li><p><strong>Optional: Configure the additional OCR settings to optimize detection and recognition:</strong></p>

<ul>
<li><a href="../class/inferenceroptions/">Inferencer Options</a></li>

<li><a href="#detectionparameters">Detection Parameters</a></li>

<li><a href="#recognitionparameters">Recognition Parameters</a>


<ul>
<li><a href="#groupersettings">Grouper Settings</a></li></ul>

<p></p></li><p></p>

<p></p><li><a href="#tilingsettings">Tiling Settings</a></li><p></p></ul><p></p></li>

<li><p><strong>Initialize the OCR object -</strong> Declare a <code>TextOCR</code> object. Use <code>CompletableFuture</code> to initialize it asynchronously with an <code>Executor</code> for concurrent processing.</p></li>

<li><p><strong>Callback Handling -</strong> Use <code>thenAccept()</code> to assign the initialized <code>TextOCR</code> object to the <code>textocr</code> variable, enabling it for text detection tasks like barcodes and products in images.</p></li>
</ol>

<h4 id="samplecode" class="anchor"><a class="heading-anchor" href="#samplecode"><span></span></a>Sample Code</h4>

<p>Initialization sample code:</p>

<pre class="prettyprint"><code>    import com.zebra.ai.vision.detector.TextOCR;     

    // Initialize the SDK 
    AIVisionSDK.getInstance(Context).init(); // Context refers to application context object. 

    //Initialize TextOCR settings object 
    String mavenModelName = "text-ocr-recognizer"; 
    TextOCR.Settings  settings = new TextOCR.Settings(mavenModelName); 

    // Optional: Override the default model input size 
    settings.detectionInferencerOptions.defaultDims.width = 1280; 
    settings.detectionInferencerOptions.defaultDims.height = 1280; 

    // Optional – Set runtime processor order to load the model
    Integer[] rpo = new Integer[3]; 
    rpo[0] = InferencerOptions.DSP;
    rpo[1] = InferencerOptions.CPU;
    rpo[2] = InferencerOptions.GPU;
    settings.detectionInferencerOptions.runtimeProcessorOrder = rpo; 
    settings.recognitionInferencerOptions.runtimeProcessorOrder = rpo; 

    // Initialize OCR object 
    TextOCR textocr = null; 

    // Initialize textocr 
    // settings = TextOCR.Settings object created above 
    // Executor = An executor thread for returning results 
    CompletableFuture&lt;TextOCR&gt; futureObject = TextOCR.getTextOCR(settings, executor); 

    // Use the futureObject to implement the thenAccept() callback of CompletableFuture 
    futureObject.thenAccept (OCRInstance -&gt; { 
        // Use the Textocr object returned here for detecting barcodes/shelves/products 
        textocr = OCRInstance; 
    }).exceptionally(e -&gt;{ 
        if (e instanceof AIVisionSDKException) { 
            Log.e(TAG, "[AIVisionSDKException] TextOCR object creation failed: " + e.getMessage()); 
        } 
        return null; 
    }); 
</code></pre>

<h3 id="step2captureimage" class="anchor"><a class="heading-anchor" href="#step2captureimage"><span></span></a>Step 2: Capture Image</h3>

<p>Capture the image and ensure the image is in the form of a Bitmap. For CameraX-based applications, developers may build their own custom ImagerAnalyzers to feed a sequence of frames to the TextOCR interface. For more information, refer to <a href="../camerax">CameraX</a>.</p>

<h3 id="step3recognizetext" class="anchor"><a class="heading-anchor" href="#step3recognizetext"><span></span></a>Step 3: Recognize Text</h3>

<p>There are two methods to recognize text within an image:</p>

<ul>
<li><strong><code>process()</code> API Method:</strong> Suitable for applications requiring both text localization and recognition in a single operation. This method is particularly well-suited for integration with frameworks like CameraX to enable a streamlined workflow where image analysis and text detection occur simultaneously. Typical Use Cases:


<ul>
<li><strong>Integration with CameraX -</strong> Used for application that utilize CameraX for image analysis. The process() method can serve as a detector for CameraX analyzers, enabling real-time text detection directly from camera feeds.</li>

<li><strong>ImageData Objects -</strong> Accepts <code>ImageData</code> objects from various sources, including CameraX, Camera2 APIs, or local storage, offering flexibility in handling input images.</li>

<li><strong>Organized Text Output -</strong> In addition to detecting text, the process() method organizes the recognized text into paragraphs, lines, and words, returning detailed <code>ParagraphEntity</code> objects.</li>

<li><strong>Localization and Recognition -</strong> Ideal for scenarios where detecting and recognizing text paragraphs in one step is required, simplifying the process and improving efficiency.</li></ul>
</li>

<li><strong><code>detect()</code> API Method:</strong> Suitable for applications that require straightfoward text detection without detailed structural information or for those working directly with bitmap images. This method offers a simpler interface for retrieving processed results asynchronously. Typical Use Cases:


<ul>
<li><strong>Bitmap Images-</strong> For applications that primarily handle bitmap images, the <code>detect()</code> method enables direct input of bitmap data for text detection.</li>

<li><strong>Basic Detection Requirements -</strong> Suitable for scenarios where generic text, words, or paragraphs required to be detected without additional structural details.</li>

<li><strong>Asynchronous Processing -</strong> Supports asynchronous detection operations using executors, making it well-suited for applications that perform background processing.</li></ul>
</li>
</ul>

<p>Choose one of these methods to recognize text within an image.</p>

<h4 id="method1usingprocessapi" class="anchor"><a class="heading-anchor" href="#method1usingprocessapi"><span></span></a>&gt; Method 1: Using Process() API</h4>

<p>The <code>process()</code> method in the TextOCR class enables applications to pass an <code>ImageData</code> object and perform both text localization and recognition in a single operation, based on the provided settings. This interface is designed to function as a "detector" for CameraX analyzers and can be used alongside other detectors, such as the <code>BarcodeDecoder</code>.</p>

<p>Note:
Applications can use the process() API even if they are not implementing the CameraX ImageAnalyzer interface. ImageData objects from other sources, such as Camera2 APIs or local storage, can also be passed to the process() API. In such cases, skip steps a and b below.</p>

<p><strong>Steps to Use the process() Method:</strong></p>

<ol>
<li><strong>Implement <code>ImageAnalysis.Analyzer</code> -</strong> Develop a custom CameraX analyzer by implementing the <code>ImageAnalysis.Analyzer</code> interface.</li>

<li><strong>Override <code>analyze()</code> -</strong> CameraX continuously feeds frames to the analyzers that are bound to it. Override the analyze() method to define the specific functionalities required for your application.</li>

<li><strong>Prepare Inputs -</strong> The <code>process()</code> method requires an <code>ImageData</code> object. Use the helper methods provided by <code>ImageData</code> to convert source image types (e.g., ImageProxy, Android.media.image, or Bitmap) into the required format.</li>

<li><strong>Localize and Decode Paragraphs -</strong> Use the <code>process()</code> method to detect and decode paragraphs. The method outputs a <code>ParagraphEntity</code> object.</li>

<li><strong>Handle Results -</strong> Once the CompletableFuture completes, process the decoded paragraph. From the <code>ParagraphEntity</code> object, extract <code>LineEntity</code> lines using the <code>getLineEntities()</code> method. Similarly, extract words from the <code>LineEntity</code> object using the <code>getWordEntities()</code> method.</li>

<li><strong>Dispose of the Decoder -</strong> After decoding is complete and the <code>TextOCR</code> instance is no longer needed, dispose of the instance to release resources.</li>
</ol>

<p>Sample Code:</p>

<pre class="prettyprint"><code>    List&lt;ParagraphEntity&gt; resultList = textocr.process(ImageData.fromImageProxy(image)).get(); 
    for (ParagraphEntity entity: resultList) { 

        // Access detection confidence 
        float confidence = entity.getAccuracy(); 

        // Access bounding box 
        Rect boundingBox = entity. getBoundingBox(); 

        // Iterate over list of paragraph entity 
        for (ParagraphEntity entity : list) { 
            // Access lines from paragraph entity 
            Line[] lines = entity.getTextParagraph().lines; 

            // Iterate over list of lines 
            for (Line line : lines) {             
                // Access words from lines entity 
                for (Word word : line.words) { 
                    //Access the Bounding Box of the Word 
                    ComplexBBox bbox = word.bbox; 

                    //Access the text of the Word 
                    DecodedText[] decodedTexts = word.decodes; 

                    // Get the decoded text with highest accuracy at first index 
                    String decodedValue = word.decodes[0].content; 
                } 
            } 
        } 
    } 
</code></pre>

<h4 id="method2usingdetectapi" class="anchor"><a class="heading-anchor" href="#method2usingdetectapi"><span></span></a>&gt; Method 2: Using detect() API</h4>

<p>Use of detect() API allows a bitmap image to be passed and the processed results to be retrieved asynchronously as <a href="../types/#complexbbox">ComplexBoundingBox</a> objects. This can then be parsed in the desired format – as generic text, words or paragraphs.</p>

<ul>
<li><p><strong>Generic Text -</strong> Outputs text in <a href="../types/#complexbbox">complex bounding boxes</a>. </p>

<p>Sample code:</p>

<pre class="prettyprint"><code>Bitmap image = ... // Your bitmap image here 

// Initialize executor 
Executor executor = Executors.newFixedThreadPool(1); 

// Input parameters include a bitmap image and an executor thread object for performing detections 
CompletableFuture&lt;OCRResult[]&gt; futureResult = textocr.detect(bitmap,executor); 

futureResult.thenAccept (ocrResults -&gt; { 
    // Process the returned output that contains complex bounding boxes and text within 

<pre><code>if (e instanceof AIVisionSDKException) { 
    Log.e(TAG, "[AIVisionSDKException] Error in text detection: " + e.getMessage()); 
} 
return null; 
</code></pre>

}); 

// Once finished with the textOCR object, dispose of it to release resources and memory used during detection. 
textOCR.dispose(); 
</code></pre></li>

<li><p><strong>Words –</strong> Outputs an array of <a href="../types/#word">words</a>. A word is a discrete unit of text identified within an image, typically separated by spaces or punctuation. </p>

<p>Sample code:</p>

<pre class="prettyprint"><code>Bitmap image = ... // Your bitmap image here 

// Initialize executor 
Executor executor = Executors.newFixedThreadPool(1); 

// Input parameters include a bitmap image and an executor thread object for performing detections 
CompletableFuture&lt;Word[]&gt; futureWords = textocr.detectWords(bitmap,executor); 

futureWords.thenAccept (words -&gt; { 
    // Process the returned array of detected words 

<pre><code>if (e instanceof AIVisionSDKException) { 
        Log.e(TAG, "[AIVisionSDKException] Error in text detection: " + e.getMessage()); 
} 
return null; 
</code></pre>

}); 

// Once finished with the textOCR object, dispose of it to release resources and memory used during detection 
textOCR.dispose(); 
</code></pre></li>

<li><p><strong>Paragraphs -</strong> Outputs a hierarchical structure of <a href="../types/#textparagraph">paragraphs</a> using the grouping mechanism described in <a href="#textocrgroupersettings">Grouper Settings</a>. A paragraph is formed by grouping words that appear on the same line, and these lines are then organized into paragraphs. The process is parameterized, with relevant parameters detailed in the Grouper Settings. </p>

<p>Sample code:</p>

<pre class="prettyprint"><code>Bitmap image = ... // Your bitmap image here 

// Initialize executor 

Executor executor = Executors.newFixedThreadPool(1); 

// Input parameters include a bitmap image and an executor thread object for performing detection 
CompletableFuture&lt;Paragraph[]&gt; futureTextParagraph = textOCR.detectParagraphs(bitmap,executor); 

futureTextParagraph.thenAccept (paragraphs -&gt; { 
    // Process the returned array of detected paragraphs. 

<pre><code>if (e instanceof AIVisionSDKException) { 
        Log.e(TAG, "[AIVisionSDKException] Error in text detection: " + e.getMessage()); 
} 
return null; 
</code></pre>

}); 

// Once finished with the textOCR object, dispose of it to release resources and memory used during detection 
textOCR.dispose(); 
</code></pre></li>
</ul>

<hr>

<h3 id="bestpractices" class="anchor"><a class="heading-anchor" href="#bestpractices"><span></span></a>Best Practices</h3>

<p>This section provides recommendations to improve recognition accuracy across a variety of use cases, from special characters and long words to handwritten text and numeric data. Strategic adjustments to input size, tiling, ROI, and other OCR settings can significantly enhance performance while balancing processing time and application requirements. </p>

<ul>
<li><p><strong>Improving Recognition Accuracy of Special Characters (e.g., '$') -</strong> Enable tiling and use higher resolutions to provide the model with more detailed input for processing. </p></li>

<li><p><strong>Recognizing Isolated Characters in Confined Spaces -</strong> Increase the model input size and enable tiling for reliable detection of isolated characters, such as those within square boxes.</p></li>

<li><p><strong>Handling Long Words and Numbers -</strong> Use larger input sizes and enable tiling to ensure complete detection of lengthy text strings (e.g., 20 to 45 characters) and improve recognition accuracy. Although enabling tiling may increase processing time, its benefits are:</p>

<ul>
<li>Enhances detection of numbers, such as images of analog meters by helping to align and cover text within the display more accurately. </li>

<li>Reduces noise and improves accuracy if the OCR feature outputs junk data, especially in images with cluttered or overlapping text elements.</li>

<li>Enhances the mode's ability to handle text beyond typical recognition limits.
Balancing higher resolutions and larger input sizes with processing time is crucial to meet application needs without unnecessary delays. Increased accuracy often requires longer processing, so finding the right balance is essential.  </li></ul></li>

<li><p><strong>Improving Text Detection on Cylindrical Objects (e.g., Coca-Cola tin bottle)-</strong> Use the Region of Interest (ROI) technique to focus on specific areas of uneven surfaces, enhancing accuracy. If values are not accurate when reading from a distance (e.g., 3 feet or more), increase the input size for better precision, ensuring the model captures finer details necessary for accurate recognition at greater distances. If special characters and alphabets are not consistently appearing, adjust the minimum box size and box threshold to improve the detection of isolated characters and reduce ambiguity.</p></li>

<li><p><strong>Improving Accuracy for Consecutive Handwritten Characters -</strong> Modify the unclip ratio to ensure accurate alignment and representation of character sequences. For incorrect numeric values decoded in OCR (e.g., tires), review and fine-tune OCR settings for numeric data accuracy.</p></li>
</ul>

<hr>

<h2 id="methods" class="anchor"><a class="heading-anchor" href="#methods"><span></span></a>Methods</h2>

<h3 id="textocrsettingssettings" class="anchor"><a class="heading-anchor" href="#textocrsettingssettings"><span></span></a>TextOCR (Settings settings)</h3>

<pre class="prettyprint"><code>    TextOCR.TextOCR(Settings settings) throws IOException
</code></pre>

<p><strong>Description:</strong> Initializes the OCR with the specified settings, allowing subsequent text detection and analysis on image inputs. It checks for the necessary model file and verifies the integrity of the archive. If issues are detected, appropriate exceptions are thrown.</p>

<p><strong>Parameters:</strong></p>

<ul>
<li><strong>settings TextOCR.Settings -</strong> An instance of the <code>Settings</code> class containing configuration options for the OCR engine.</li>
</ul>

<p><strong>Return Value:</strong> <code>CompletableFuture</code>&lt;TextOCR&gt;</p>

<p><strong>Exceptions:</strong></p>

<ul>
<li><strong>IOException -</strong> Thrown if the archive is corrupted.</li>
</ul>

<hr>

<h3 id="detectbitmapsrcimgexecutorexecutor" class="anchor"><a class="heading-anchor" href="#detectbitmapsrcimgexecutorexecutor"><span></span></a>detect (Bitmap srcImg, Executor executor)</h3>

<pre class="prettyprint"><code>    CompletableFuture&lt;OCRResult[]&gt; detect (Bitmap srcImg, Executor executor) throws InvalidInputException, AIVisionSDKException
</code></pre>

<p><strong>Description:</strong> Performs Optical Character Recognition (OCR) on the provided Bitmap image, using the specified executor for asynchronous execution.</p>

<p><strong>Parameters:</strong></p>

<ul>
<li><strong>srcImg (Bitmap srcImg) -</strong> The Bitmap image to perform OCR on.</li>

<li><strong>executor -</strong> Manages asynchronous task execution.</li>
</ul>

<p><strong>Return Value:</strong> A <code>CompletableFuture</code> that resolves to an array of <a href="../types/#ocrresult">OCRResult</a>, each containing <a href="../types/#complexbbox">complex bounding boxes</a> and recognized text.</p>

<p><strong>Exceptions:</strong></p>

<ul>
<li><strong>InvalidInputException -</strong> Thrown if the Bitmap is null.</li>

<li><strong>AIVisionSDKException -</strong> Thrown if error in detection or image queue is full.</li>
</ul>

<hr>

<h3 id="detectwordsbitmapsrcimgexecutorexecutor" class="anchor"><a class="heading-anchor" href="#detectwordsbitmapsrcimgexecutorexecutor"><span></span></a>detectWords (Bitmap srcImg, Executor executor)</h3>

<pre class="prettyprint"><code>    CompletableFuture&lt;Word[]&gt; TextOCR.detectWords (Bitmap srcImg, Executor executor) throws InvalidInputException, AIVisionSDKException
</code></pre>

<p><strong>Description:</strong> Detects individual words in the provided Bitmap image using the specified executor for asynchronous execution.</p>

<p><strong>Parameters:</strong></p>

<ul>
<li><strong>srcImg (Bitmap srcImg) -</strong> The image to analyze for word detection.</li>

<li><strong>Executor -</strong> Manages asynchronous task execution.</li>
</ul>

<p><strong>Return Value:</strong> A <code>CompletableFuture</code> that resolves to an array of Word objects, each containing <a href="../types/#complexbbox">complex bounding boxes</a> and possible text decodes.</p>

<p><strong>Exceptions:</strong></p>

<ul>
<li><strong>InvalidInputException -</strong> Thrown if the Bitmap is null.</li>

<li><strong>AIVisionSDKException -</strong> Thrown if there is an error in detection or the image queue is full.</li>
</ul>

<hr>

<h3 id="detectparagraphsbitmapsrcimgexecutorexecutor" class="anchor"><a class="heading-anchor" href="#detectparagraphsbitmapsrcimgexecutorexecutor"><span></span></a>detectParagraphs (Bitmap srcImg, Executor executor)</h3>

<pre class="prettyprint"><code>    CompletableFuture&lt;TextParagraph[]&gt; detectParagraphs(Bitmap srcImg, Executor executor) throws InvalidInputException, AIVisionSDKException
</code></pre>

<p><strong>Description:</strong> Detects paragraphs in the provided Bitmap image using the specified executor for asynchronous execution.</p>

<p><strong>Parameters:</strong></p>

<ul>
<li><strong>srcImg (Bitmap srcImg) -</strong> The image to analyze for paragraph detection.</li>

<li><strong>executor -</strong> Manages asynchronous task execution.</li>
</ul>

<p><strong>Return Value:</strong> A <code>CompletableFuture</code> that resolves to an array of TextParagraph objects, representing detected paragraphs.</p>

<p><strong>Exceptions:</strong></p>

<ul>
<li><strong>InvalidInputException -</strong> Thrown if the Bitmap is null.</li>

<li><strong>AIVisionSDKException -</strong> Thrown if the AI Data Capture SDK is not initialized.</li>
</ul>

<hr>

<h3 id="gettextocrsettingssettingsexecutorexecutor" class="anchor"><a class="heading-anchor" href="#gettextocrsettingssettingsexecutorexecutor"><span></span></a>getTextOCR (Settings settings, Executor executor)</h3>

<pre class="prettyprint"><code>    CompletableFuture&lt;TextOCR&gt; getTextOCR(Settings settings, Executor executor) throws InvalidInputException, AIVisionSDKSNPEException, AIVisionSDKException, AIVisionSDKModelException, AIVisionSDKLicenseException
</code></pre>

<p><strong>Description:</strong> Asynchronously initializes and retrieves a TextOCR instance using the specified settings and executor.</p>

<p><strong>Parameters:</strong></p>

<ul>
<li><strong>Settings -</strong> An instance of <code>TextOCR.Settings</code> containing configuration options for the OCR engine.</li>

<li><strong>executor -</strong> Manages asynchronous task execution.</li>
</ul>

<p><strong>Return Value:</strong> A <code>CompletableFuture</code> that resolves to an initialized TextOCR instance.</p>

<p><strong>Exceptions:</strong></p>

<ul>
<li><strong>InvalidInputException -</strong> Thrown if the settings are invalid or null.</li>

<li><strong>AIVisionSDKSNPEException -</strong> Thrown if SNPE fails to initialize. </li>

<li><strong>AIVisionSDKException -</strong> Thrown if the AI Vision SDK is not initialized. </li>

<li><strong>AIVisionSDKModelException -</strong> Thrown if the current SDK version is below the minimum required version. To resolve this, update the SDK to a newer or the latest version.</li>

<li><strong>AIVisionSDKLicenseException -</strong> Thrown if there are licensing issues related to the text-ocr-recognizer model. </li>
</ul>

<hr>

<h3 id="processimagedataimagedataexecutorexecutor" class="anchor"><a class="heading-anchor" href="#processimagedataimagedataexecutorexecutor"><span></span></a>process (ImageData imageData, Executor executor)</h3>

<pre class="prettyprint"><code>    CompletableFuture&lt;List&lt;ParagraphEntity&gt;&gt; process(ImageData imageData, Executor executor) throws AIVisionSDKException 
</code></pre>

<p>Processes an image to detect text paragraphs, organizing the detected text into words, lines, and paragraphs. This method executes asynchronously and returns a CompletableFuture that can be used to retrieve the results once they are available. </p>

<p><strong>Parameters:</strong></p>

<ul>
<li><strong>imageData -</strong> The image data to be processed for text detection. </li>

<li><strong>executor -</strong> Results are returned in this executor. </li>
</ul>

<p><strong>Return Value:</strong></p>

<ul>
<li><strong>CompletableFuturet&lt;List&lt;ParagraphEntity&gt;&gt; -</strong> A future that completes with a list of <code>ParagraphEntity</code> objects representing the detected text. </li>
</ul>

<p><strong>Exceptions:</strong></p>

<ul>
<li><strong>AIVisionSDKException -</strong> Thrown if the image data is <code>null</code> or if the previous detection is still in progress. </li>
</ul>

<hr>

<h3 id="processimagedataimagedata" class="anchor"><a class="heading-anchor" href="#processimagedataimagedata"><span></span></a>process(ImageData imageData)</h3>

<pre class="prettyprint"><code>    CompletableFuture&lt;List&lt;ParagraphEntity&gt;&gt; process(ImageData imageData) throws InvalidInputException, AIVisionSDKException 
</code></pre>

<p>Processes an image to detect text paragraphs and organizes text into words, lines, and paragraphs. </p>

<p><strong>Parameters:</strong></p>

<ul>
<li><strong>imageData -</strong> The image data to be processed. </li>
</ul>

<p><strong>Return Value:</strong>
Returns a CompletableFuture&lt;List&lt;ParagraphEntity&gt;&gt; containing the detected text. </p>

<p><strong>Exceptions:</strong></p>

<ul>
<li><strong>InvalidInputException -</strong> Thrown if <code>imageData</code> is null. </li>

<li><strong>AIVisionSDKException -</strong> Thrown if the previous detection is still in progress. </li>
</ul>

<hr>

<h3 id="dispose" class="anchor"><a class="heading-anchor" href="#dispose"><span></span></a>dispose()</h3>

<pre class="prettyprint"><code>    void dispose()
</code></pre>

<p><strong>Description:</strong> Releases all internal resources used by the TextOCR object. This function must be called manually to free up resources.</p>

<hr>

<h2 id="textocrsettings" class="anchor"><a class="heading-anchor" href="#textocrsettings"><span></span></a>TextOCR.Settings</h2>

<p>The <code>Settings</code> class is a nested class within the <code>TextOCR class</code>, which leverages Optical Character Recognition (OCR) to detect, recognize, and group text from images. The flexibility of its parameters allows developers to fine-tune performance for diverse use cases, including document scanning, real-time recognition, and automated data entry.  </p>

<hr>

<h3 id="constructors" class="anchor"><a class="heading-anchor" href="#constructors"><span></span></a>Constructors</h3>

<h4 id="settingsstringmavenmodelname" class="anchor"><a class="heading-anchor" href="#settingsstringmavenmodelname"><span></span></a>Settings(String mavenModelName)</h4>

<pre class="prettyprint"><code>    TextOCR.Settings textOCRSettings = new TextOCR.Settings(mavenModelName) throws InvalidInputException,AIVisionSDKException; 
</code></pre>

<p><strong>Description:</strong> Constructor for the <code>Settings</code> object with model name.</p>

<p><strong>Parameters:</strong></p>

<ul>
<li><strong>mavenModelName -</strong> The name of the model specified in the Maven repository. </li>
</ul>

<p><strong>Exceptions:</strong></p>

<ul>
<li><strong>InvalidInputException -</strong> Thrown if the mavenModelName is invalid. </li>

<li><strong>AIVisionSDKException -</strong> Thrown if an error occurs while reading the specified model or the AI Data Capture SDK is not initialized. </li>
</ul>

<h4 id="settingsfilemodelfile" class="anchor"><a class="heading-anchor" href="#settingsfilemodelfile"><span></span></a>Settings(File ModelFile)</h4>

<pre class="prettyprint"><code>    TextOCR.Settings textOCRSettings = new TextOCR.Settings(modelFile) throws InvalidInputException,AIVisionSDKException; 
</code></pre>

<p><strong>Description:</strong> Constructs a new Settings object with File object passed. </p>

<p><strong>Parameters:</strong></p>

<ul>
<li><strong>ModelFile -</strong> The file object that contains the Text OCR model. </li>
</ul>

<p><strong>Exceptions:</strong> </p>

<ul>
<li><strong>InvalidInputException -</strong> Thrown if the modelFile is invalid. </li>

<li><strong>AIVisionSDKException -</strong> Thrown if an error occurs while reading the specified model or the AI Data Capture SDK is not initialized. </li>
</ul>

<hr>

<h2 id="textdetection" class="anchor"><a class="heading-anchor" href="#textdetection"><span></span></a>Text Detection</h2>

<p>The Detection phase processes the input image to create <a href="../types/#complexbbox">complex bounding boxes</a>, or text boxes. Each text box is represented by a list of points forming a rotated rectangle, which may not be perfectly aligned with the screen’s edges. There may be more than four points if the rectangle is clipped at the edges of the screen. Adjusting Detection Parameters allows for improved accuracy, catering to specific use cases like document scanning, real-time text recognition, or automated data entry. </p>

<p>Typical scenarios for adjusting Detection Parameters:</p>

<ul>
<li><strong>Document Scanning:</strong> Digitize documents by extracting text for storage and retrieval.</li>

<li><strong>Real-Time Text Recognition:</strong> Integrate into applications requiring immediate text recognition from images or video streams.</li>

<li><strong>Automated Data Entry:</strong>  Simplify workflows by pulling text from forms, invoices, or other structured documents. </li>
</ul>

<hr>

<h3 id="detectionparameters" class="anchor"><a class="heading-anchor" href="#detectionparameters"><span></span></a>Detection Parameters</h3>

<p>To refine detection accuracy, adjust the <strong>Detection Parameters</strong>.</p>

<hr>

<h4 id="detectioninferenceroptions" class="anchor"><a class="heading-anchor" href="#detectioninferenceroptions"><span></span></a>detectionInferencerOptions</h4>

<pre class="prettyprint"><code>    InferencerOptions TextOCR.Settings.detectionInferencerOptions = new InferencerOptions()
</code></pre>

<p><strong>Description:</strong> Allows developers to specify a different input shape for the detection stage inferencer.</p>

<hr>

<h4 id="recognitioninferenceroptions" class="anchor"><a class="heading-anchor" href="#recognitioninferenceroptions"><span></span></a>recognitionInferencerOptions</h4>

<pre class="prettyprint"><code>    InferencerOptions TextOCR.Settings.recognitionInferencerOptions = new InferencerOptions()
</code></pre>

<p><strong>Description:</strong> Typically remains unchanged as the input size is fixed for the recognition model. If needed, Recognition results can be adjusted using parameters in the <a href="#recognitionparameters">Recognition Parameters</a> section.
<strong>Note:</strong> These options should not be changed by the developer.</p>

<hr>

<h3 id="detectionprocess" class="anchor"><a class="heading-anchor" href="#detectionprocess"><span></span></a>Detection Process</h3>

<p>The detection process operates in two main stages: </p>

<ol>
<li><strong>Heatmap Threshold (Pixel-Level Filtering) -</strong> Filters pixels based on their likelihood of being part of text. A heatmap is generated where each pixel is assigned a score indicating the likelihood of it being part of a text character. The Heatmap Threshold filters out pixels with low scores, retaining only the most probable candidates for further processing. </li>

<li><strong>Box Threshold (Box-Level Filtering) -</strong> Groups filtered the pixels into bounding boxes and removes low-confidence detections. After pixel filtering, the system identifies groups of pixels and draws bounding boxes around them. Each box is assigned a confidence score, and the Box Threshold filters out boxes with low confidence, retaining only those likely to contain text. </li>
</ol>

<p>Once potential text boxes are identified, additional filtering can be applied to refine results. This includes adjusting box size, area, and orientation to eliminate noise or unwanted detections and optimizing the detection for accurate text recognition. These refinements are acheived using <a href="#filteringparameters">Filtering Parameters</a>.</p>

<hr>

<h4 id="heatmapthreshold" class="anchor"><a class="heading-anchor" href="#heatmapthreshold"><span></span></a>heatmapThreshold</h4>

<pre class="prettyprint"><code>    Float TextOCR.Settings.heatmapThreshold
</code></pre>

<p><strong>Description:</strong> Sets a cutoff to identify potential areas likely to contain text, converting them into text boxes. (Internally, the detector model creates a grayscale image, or heatmap, that represents text confidence.) </p>

<p><strong>Tuning effect:</strong></p>

<ul>
<li><strong>Increase Threshold -</strong> Reduces areas identified as text and reduces noise. Useful for high-contrast clear text such as scanned documents.</li>

<li><strong>Decrease Threshold -</strong> Expands areas identified as text. Useful for faint, curved, or blurred text with low contrast. </li>
</ul>

<p><strong>Default:</strong> 0.5f</p>

<p><strong>Valid range:</strong> [0.0f, 1.0f]</p>

<hr>

<h4 id="boxthreshold" class="anchor"><a class="heading-anchor" href="#boxthreshold"><span></span></a>boxThreshold</h4>

<pre class="prettyprint"><code>    Float TextOCR.Settings.boxThreshold
</code></pre>

<p><strong>Description:</strong> Sets the minimum confidence score required for a text box to be included in the OCR output. After pixel filtering, the system identifies pixel groups and draws bounding boxes around them, assigning each box a confidence score. The <strong>Box Threshold</strong> is then applied to filter out boxes with low confidence, retaining only those likely to contain text, thereby improving detection accuracy.</p>

<!-- 
Sets the minimum confidence score required for a text box to be included in the OCR output. Boxes with confidence scores below this threshold are excluded, helping to filter out less certain text detections.
-->

<p><strong>Tuning effect:</strong></p>

<ul>
<li><strong>Increase Threshold:</strong> Excludes less-confident text boxes (reduces false positives), useful when too many boxes are detected.</li>

<li><strong>Decrease Threshold:</strong> Includes more text boxes (catches weak detections), which might be necessary when important text is being missed.</li>
</ul>

<p><strong>Default:</strong> 0.85f</p>

<p><strong>Valid range:</strong> [0, 1.0]</p>

<hr>

<h3 id="filteringparameters" class="anchor"><a class="heading-anchor" href="#filteringparameters"><span></span></a>Filtering Parameters</h3>

<h4 id="minboxarea" class="anchor"><a class="heading-anchor" href="#minboxarea"><span></span></a>minBoxArea</h4>

<pre class="prettyprint"><code>    Integer TextOCR.Settings.minBoxArea
</code></pre>

<p><strong>Description:</strong> Filters out text boxes if their total area (width × height) is too small, filtering "tiny" boxes. This helps remove unimportant boxes from the OCR output.</p>

<p><strong>Tuning effect:</strong> </p>

<ul>
<li><strong>Increase Parameter:</strong> Filters out boxes with small areas and eliminate dust, dots, or tiny artifacts. </li>

<li><strong>Decrease Parameter:</strong> Helps to detect smaller text.</li>
</ul>

<p><strong>Default:</strong> 10</p>

<p><strong>Valid range:</strong> [0, max(int)]</p>

<hr>

<h4 id="minboxsize" class="anchor"><a class="heading-anchor" href="#minboxsize"><span></span></a>minBoxSize</h4>

<pre class="prettyprint"><code>    Integer TextOCR.Settings.minBoxSize
</code></pre>

<p><strong>Description:</strong> Filters out text boxes that are too narrow ("skinny") or too short that likely do not contain real text.</p>

<p><strong>Tuning effect:</strong> </p>

<ul>
<li><strong>Increase Parameter:</strong> Filters out very narrow boxes and helps ignore divider lines, underscores, or non-text lines </li>

<li><strong>Decrease Parameter:</strong> Helps to detect smaller text.</li>
</ul>

<p><strong>Default:</strong> 1</p>

<p><strong>Valid range:</strong> [0, max(int)]</p>

<hr>

<h4 id="minratioforrotation" class="anchor"><a class="heading-anchor" href="#minratioforrotation"><span></span></a>minRatioForRotation</h4>

<pre class="prettyprint"><code>    Float TextOCR.Settings.minRatioForRotation
</code></pre>

<p><strong>Description:</strong> Rotates vertically (high height, low width) oriented boxes so they become horizontal. <br>
<strong>Note:</strong> Words are generally wider than they are tall, so their ratio should exceed the default value. Therefore, avoid changing this parameter for words, since word complex bounding boxes should be horizontally oriented before recognition.</p>

<p><strong>Tuning effect:</strong> Setting this parameter to 0 disables rotation. Otherwise, rotate boxes with a height-to-width ratio exceeding this value 90 degrees counterclockwise before recognition.</p>

<p><strong>Default:</strong> 1.5f</p>

<p><strong>Valid range:</strong> [0.0f, inf] (where ‘inf’ denotes infinity)</p>

<hr>

<h4 id="unclipratio" class="anchor"><a class="heading-anchor" href="#unclipratio"><span></span></a>unclipRatio</h4>

<pre class="prettyprint"><code>    float TextOCR.Settings.unclipRatio
</code></pre>

<p><strong>Description:</strong> Expands or "stretches" detected boxes outward to include full characters and some background. Expands box size before recognition to improve results. Tight-fitting boxes might benefit from some extra background for better decoding.</p>

<p><strong>Tuning effect:</strong> Increasing this parameter enlarges text boxes, potentially improving recognition. An <code>unclipRatio</code> of 1 keeps boxes unchanged, while 1.5 enlarges them by 50%.</p>

<ul>
<li><strong>Increase Parameter:</strong> For curved, rotated, or incomplete detections </li>

<li><strong>Decrease Parameter:</strong> To avoid overlapping with neighboring text regions or noisy regions </li>
</ul>

<p><strong>Default:</strong> 1.5f</p>

<p><strong>Valid range:</strong> [1.0f, inf]</p>

<hr>

<h3 id="samplecode-1" class="anchor"><a class="heading-anchor" href="#samplecode-1"><span></span></a>Sample Code</h3>

<p>This sample code demonstrates how to adjust detection parameter settings:</p>

<ol>
<li><p><strong>Configure Settings:</strong> Initialize a <code>TextOCR.Settings</code> object and customize parameters such as <code>heatmapThreshold</code> and <code>boxThreshold</code> to improve detection accuracy based on your specific needs.</p></li>

<li><p><strong>Asynchronous Initialization:</strong> Use an <code>Executor</code> to initialize the <code>TextOCR</code> instance asynchronously, allowing for efficient resource management and responsiveness.</p></li>

<li><p><strong>Load Bitmap Image:</strong> Prepare the image for OCR by converting it to a Bitmap object.</p></li>

<li><p><strong>Perform OCR:</strong> Use the <code>detect</code> method to analyze the image and retrieve an array of <a href="../types/#ocrresult">OCRResult</a> objects with complex bounding boxes and recognized text.</p></li>

<li><p><strong>Process OCR Results:</strong> Handle the results by iterating over the <code>OCRResult</code> array, outputting the recognized text or using it for further processing.</p></li>

<li><p><strong>Dispose Resources:</strong> After completing OCR operations, call <code>dispose()</code> to release resources and prevent memory leaks.</p>

<pre class="prettyprint"><code>import com.zebra.ai.vision.TextOCR; 
import com.zebra.ai.vision.TextOCR.Settings; 
import android.graphics.Bitmap; 

// Initialize settings with a custom heatmap threshold 
String mavenModelName = "text-ocr-recognizer"; 
TextOCR.Settings textOCRSettings settings = new TextOCR.Settings (mavenModelName); 
settings.heatmapThreshold = 0.3f; // Lower threshold for low-contrast text 
settings.boxThreshold = 0.9f; // Higher threshold for more confident text boxes 
settings.minBoxSize = 10; // Set minimum box size to 10 pixels 
settings.minBoxArea = 50; // Set minimum box area to 50 pixels 
settings.unclipRatio = 2.0f; // Enlarge text boxes by 100% 
settings.minRatioForRotation = 2.0f; // Rotate boxes with height-to-width ratio exceeding 2.0 

// Optional – set runtime processor order, by default {DSP, CPU, GPU} is used          
Integer[] rpo = new Integer[3];  
rpo[0] = InferencerOptions.DSP;     
rpo[1] = InferencerOptions.CPU; 
rpo[2] = InferencerOptions.GPU; 
settings.detectionInferencerOptions.runtimeProcessorOrder = rpo; 
settings.recognitionInferencerOptions.runtimeProcessorOrder = rpo; 

// Initialize executor 
Executor executor = Executors.newFixedThreadPool(1); 

CompletableFuture&lt;TextOCR&gt; futureObject = TextOCR. getTextOCR(settings, executor); 

// Use the futureObject to implement thenAccept() callback of CompletableFuture 
futureObject.thenAccept (OCRInstance -&gt; { 
    // Use the Textocr object returned here for the detection of barcodes/shelves/products 
    textocr = OCRInstance; 
}); 

// Load your Bitmap image 
Bitmap image = ...; // Your input image 

// Perform OCR 
CompletableFuture&lt;Result[]&gt; futureResult = textocr.detect(bitmap,executor); 
futureResult.thenAccept (ocrResults -&gt; { 
    // Process the returned output that contains complex bounding boxes and text in it 
}); 

// Dispose resources 
// Once use of the textOCR object is done, dispose it to release the resources and memory used for detection 
textOCR.dispose(); 
</code></pre></li>
</ol>

<hr>

<h2 id="textrecognition" class="anchor"><a class="heading-anchor" href="#textrecognition"><span></span></a>Text Recognition</h2>

<p>The Recognition stage analyzes the text within each <a href="../types/#complexbbox">complex bounding box</a>, or text box, produced during the Detection Stage to identify the text content. Each text box results in a list of potential text decodes. </p>

<p>After text boxes are detected, the next step is to extract and accurately read the text within each bounding box. AI Suite uses the <strong>"Total" decoder</strong> to convert character predictions into meaningful words, even in cases where the model is uncertain about specific characters. </p>

<p>The <strong>"Total" decoder</strong> employs a systematic filtering process to refine character predictions, focusing on balancing accuracy and efficiency while assembling words. Adjusting the Decoder Parameters TopK Ignore Cutoff, Total Prob Threshold, and Max Word Combinations, act as filters to refine predictions and determine the final output. </p>

<p>Step-by-Step Process:</p>

<ol>
<li><strong>Generate a Ranked List of Predictions for Each Character:</strong> For every character slot (e.g., a space in a word), the system creates a list of possible characters, ranked by confidence scores. 


<ul>
<li>To explain how the parameters work together, consider the following example predictions to apply for this process: 'S' at 40%, 's' at 30%, '5' at 15%, 'B' at 5%, '8' at 2% </li></ul>
</li>

<li><strong>Apply Two Filters to Refine Predictions:</strong>


<ul>
<li><strong>First Filter -</strong> TopK Ignore Cutoff (The Gatekeeper): Limits how many of the highest-confidence character predictions are considered for each character slot. 


<ul>
<li><strong>Example:</strong> If the cutoff is 4, only the top 4 predictions ('S', 's', '5', 'B') are kept. Predictions below the cutoff (like '8') are discarded.  </li></ul>
</li>

<li><strong>Second Filter -</strong> Total Prob Threshold (The Quality Check): Ensures the cumulative confidence of the retained predictions meets a defined minimum threshold (e.g., 90%). 


<ul>
<li><strong>Example:</strong> Using the top predictions ('S', 's', '5', 'B'), their combined confidence is: 0.40 + 0.30 + 0.15 + 0.05 = 0.90. 


<ul>
<li>If the combined score is below the threshold (e.g., 85%), the system gives up on this character slot and outputs a placeholder like "".  </li>

<li>If the score meets or exceeds the threshold, the decoder narrows down predictions further (e.g., keeping only 'S' and 's' if a stricter threshold like 0.50 is used).  </li></ul>
</li></ul>
</li></ul>
</li>

<li><strong>Word Assembly:</strong> Once character predictions pass the filters, they are sent to the next stage: assembling them into valid words. This limits the number of full-word combinations generated from the remaining character predictions after filtering. 


<ul>
<li><strong>Example:</strong> After filtering, 20 valid word combinations remain. If Max Word Combinations is set to 5, only the top 5 most confident word results are returned. The remaining 15 combinations are ignored, even if they are valid.  </li></ul>
</li>
</ol>

<hr>

<h3 id="recognitionparameters" class="anchor"><a class="heading-anchor" href="#recognitionparameters"><span></span></a>Recognition Parameters</h3>

<p>This section provides the <strong>Recognition Parameters</strong> to help refine the recognition process.</p>

<hr>

<h4 id="decodingtopkignorecutoff" class="anchor"><a class="heading-anchor" href="#decodingtopkignorecutoff"><span></span></a>decodingTopkIgnoreCutoff</h4>

<pre class="prettyprint"><code>    Integer TextOCR.Settings.decodingTopkIgnoreCutoff
</code></pre>

<p><strong>Description:</strong> The maximum number of highest-confidence character predictions the "Total" decoder considers for each character position, impacting the accuracy and completeness of text recognition. If additional characters are needed to meet the Total Prob Threshold, the model outputs a replacement character (e.g., "�"). This parameter is applicable for the following scenarios:</p>

<ul>
<li><strong>Complex Text Recognition -</strong> Increase this parameter for documents with complex or ambiguous text where capturing all character variations is crucial.</li>

<li><strong>Improving Character Accuracy -</strong> Use this setting in scenarios where critical text components are consistently missing, ensuring thorough character analysis.</li>

<li><strong>Adaptive Text Processing -</strong> Adjust dynamically based on the complexity and quality of input text to optimize OCR performance.</li>
</ul>

<p><strong>Tuning effect:</strong> Generally, keep this at the default value. If the expected character does not appear in the OCR output, increasing this value allows for more less confident decodes.</p>

<p><strong>Default:</strong> 4</p>

<p><strong>Valid range:</strong> [1, max(int)]</p>

<hr>

<h4 id="decodingtotalprobthreshold" class="anchor"><a class="heading-anchor" href="#decodingtotalprobthreshold"><span></span></a>decodingTotalProbThreshold</h4>

<pre class="prettyprint"><code>    Float TextOCR.Settings.decodingTotalProbThreshold
</code></pre>

<p><strong>Description:</strong> Sets the minimum cumulative confidence score that character decodes must achieve to be accepted. This setting is crucial in the total decoding strategy of the OCR recognition process, as it balances accuracy and coverage in text recognition. If the threshold is not reached, no high-confidence decode exists, resulting in a placeholder character (�) appearing in the output.</p>

<p>Relevant scenarios:</p>

<ul>
<li><strong>Improving Decode Coverage -</strong> Lower the threshold when critical text characters are missing, to capture a wider range of more potential decodes.</li>

<li><strong>Analyzing Complex Documents -</strong> Apply this setting for documents with ambiguous or low-quality text to ensure more comprehensive character recognition.</li>

<li><strong>Adaptive Recognition -</strong> Adjust dynamically based on the quality and complexity of input documents to optimize OCR performance for specific needs.</li>
</ul>

<p><strong>Tuning effect:</strong> If many characters are not decoded, evidenced by multiple � characters, decreasing this value may improve results. Increase this parameter for more flexible but potentially noisier results. Decrease this parameter for more trustworthy, reliable outputs.</p>

<p><strong>Default:</strong> 0.9f</p>

<p><strong>Valid range:</strong> [0.0f, 1.0f]</p>

<hr>

<h4 id="decodingmaxwordcombinations" class="anchor"><a class="heading-anchor" href="#decodingmaxwordcombinations"><span></span></a>decodingMaxWordCombinations</h4>

<pre class="prettyprint"><code>    Integer TextOCR.Settings.decodingMaxWordCombinations
</code></pre>

<p><strong>Description:</strong> Restricts the number of valid word outputs generated from possible character combinations for each detection. This helps avoid overwhelming results, particularly for ambiguous inputs, by limiting the model’s consideration of all potential character combinations across all positions in the word. It is applicable for the following scenarios:</p>

<ul>
<li><strong>Detailed Text Analysis -</strong> Increase this parameter for applications that require a thorough analysis of text.</li>

<li><strong>Data Extraction -</strong> Adjust this parameter to optimize the extraction of comprehensive data from documents with complex or ambiguous text.</li>
</ul>

<p><strong>Tuning effect:</strong> Increasing this number returns more decodes, but potentially with lower confidence. Decreasing this parameter results to faster processing and fewer alternatives.</p>

<p><strong>Default:</strong> 10</p>

<p><strong>Valid range:</strong> [1, max(int)]</p>

<hr>

<h3 id="recognitionspecialcases" class="anchor"><a class="heading-anchor" href="#recognitionspecialcases"><span></span></a>Recognition: Special Cases</h3>

<p>These features are intended only for special scenarios and are usually not needed for most OCR tasks.</p>

<hr>

<h4 id="flip" class="anchor"><a class="heading-anchor" href="#flip"><span></span></a>flip</h4>

<pre class="prettyprint"><code>    boolean TextOCR.Settings.flip
</code></pre>

<p><strong>Description:</strong> Runs recognition in multiple orientations to boost accuracy on rotated or flipped text. If set to true, performs recognition twice - once in the regular orientation and once rotated by 180 degrees. Enable only if text orientation varies, as it increases processing time.</p>

<h4 id="tiling" class="anchor"><a class="heading-anchor" href="#tiling"><span></span></a>Tiling</h4>

<p><a href="#tiling-1">Tiling</a> helps OCR handle very long, thin lines of text (like serial numbers, document titles, or part numbers) by splitting them into smaller, manageable pieces ("tiles") for better recognition. This is useful when a word box exceeds the recognition limit (15 characters). Tiling adds processing time and should only be used as needed. </p>

<hr>

<h3 id="samplecode-2" class="anchor"><a class="heading-anchor" href="#samplecode-2"><span></span></a>Sample Code</h3>

<p>Sample code demonstrating use of recognition parameters:</p>

<ol>
<li><p><strong>Initialize Settings:</strong> Configure the OCR settings, including additional parameters such as <code>heatmapThreshold</code> and <code>tiling</code>.</p></li>

<li><p><strong>Create TextOCR Instance:</strong> Use an executor to initialize the <code>TextOCR</code> instance asynchronously with the configured settings.</p></li>

<li><p><strong>Load Bitmap Image:</strong> Prepare the bitmap image that you want to analyze using OCR.</p></li>

<li><p><strong>Perform OCR:</strong> Invoke the <code>detect</code> method on the <code>TextOCR</code> instance to analyze the bitmap image, managing the asynchronous processing with the executor.</p></li>

<li><p><strong>Process OCR Results:</strong> Handle the results, which include complex bounding boxes and recognized text.</p></li>

<li><p><strong>Dispose Resources:</strong> After completing OCR operations, call the <code>dispose</code> method on the <code>TextOCR</code> instance to release resources and prevent memory leaks.</p>

<pre class="prettyprint"><code>import com.zebra.ai.vision.TextOCR; 
import com.zebra.ai.vision.TextOCR.Settings; 
import android.graphics.Bitmap; 

// Initialize settings 
String mavenModelName = "text-ocr-recognizer"; 
TextOCR.Settings textOCRSettings = new TextOCR.Settings (mavenModelName); 
textOCRSettings.heatmapThreshold = 0.5f; 
textOCRSettings.decodingTotalProbThreshold = 0.9f; 
textOCRSettings.tiling.enable = true; 

// Optional : set runtime processing order, by default DSP will be used 
Integer[] rpo = new Integer[]{InferencerOptions.DSP}; 
textOCRSettings.detectionInferencerOptions.runtimeProcessorOrder = rpo; 
textOCRSettings.recognitionInferencerOptions.runtimeProcessorOrder = rpo; 

// Instantiate TextOCR with the configured settings 
// settings = TextOCR.Settings object created above 
// Executor = An executor thread for processing API calls and returning results 

// Initialize executor 
Executor executor = Executors.newFixedThreadPool(1); 

CompletableFuture&lt;TextOCR&gt; futureObject = TextOCR.getTextOCR(textOCRSettings, executor); 

// Use the futureObject to implement thenAccept() callback of CompletableFuture. 
futureObject.thenAccept (OCRInstance -&gt; { 
    // Use the textocr object returned here detecting barcodes, shelves, or products 
    textocr = OCRInstance; 
}); 

// Load your Bitmap image 
Bitmap image = ...; 

// Perform OCR 
CompletableFuture&lt;Result[]&gt; futureResult = textocr.detect(bitmap,executor); 
futureResult.thenAccept (Result -&gt; { 
    // Process the returned output that contains complex bounding boxes and recognized text 
}); 

// Dispose resources 
// Once done using the textOCR object, dispose it to release resources and memory used for detection. 
textOCR.dispose();
</code></pre></li>
</ol>

<hr>

<h3 id="tiling-1" class="anchor"><a class="heading-anchor" href="#tiling-1"><span></span></a>Tiling</h3>

<p>The OCR Recognition stage limits Word boxes to 15 characters. To achieve good results with “Words” containing more than 15 characters, such as ID numbers or VINs, enable Tiling. Tiling splits text boxes generated at the localization stage into overlapping crops, performs recognition on each, and uses a correlation-based merging algorithm to prepare a unified decode. Tiling increases processing time, so use it only when needed. Not all “Words” will be tiled; only those meeting threshold criteria specified by the developer will be tiled.</p>

<hr>

<h4 id="tilingsettings" class="anchor"><a class="heading-anchor" href="#tilingsettings"><span></span></a>Tiling Settings</h4>

<p>The <code>TilerSettings</code> class is a configuration component within the <code>TextRecognizer.Settings</code> framework of the Zebra AI Data Capture SDK. It provides parameters to fine-tune the behavior of the tiling feature, which is used during the text detection and recognition process. These settings primarily control how boxes are merged and processed based on their aspect ratios and correlation thresholds. </p>

<p><i class="fa fa-exclamation-triangle" style="color:#FFA500;"></i> <strong>Caution:</strong> Tiling adds processing time and should only be used as needed.</p>

<p>Configure <code>TilerSettings</code> in the following scenarios:</p>

<ul>
<li><strong>Large Document Processing:</strong> Enable tiling to process large documents efficiently, especially when sections require individual handling due to size limitations.</li>

<li><strong>Complex Layout Handling:</strong> Adjust tiling settings for documents with complex layouts to improve the accuracy of text recognition.</li>

<li><strong>Performance Tuning:</strong> Fine-tune parameters to achieve an optimal balance between processing speed and accuracy, based on specific application requirements.</li>
</ul>

<!-- 
---

#### Tiling

        TilerSettings TextOCR.Settings.tiling

**Description:** Tiling involves dividing a large image into smaller, manageable sections called "tiles" to analyze text more accurately. This structure contains parameters for handling very long text boxes.
-->

<hr>

<h4 id="enable" class="anchor"><a class="heading-anchor" href="#enable"><span></span></a>enable</h4>

<pre class="prettyprint"><code>    Boolean TextOCR.Settings.TilerSettings.enable
</code></pre>

<p><strong>Description:</strong> Enables or disables the tiling feature. When true, <code>TextOCR</code> performs tiling operations on detected text regions, splitting boxes that meet aspect ratio criteria into multiple tiles, recognizing text, and merging results using a correlation method.</p>

<p><strong>Default:</strong> false</p>

<hr>

<h4 id="aspectratiolowerthr" class="anchor"><a class="heading-anchor" href="#aspectratiolowerthr"><span></span></a>aspectRatioLowerThr</h4>

<pre class="prettyprint"><code>    Float TextOCR.Settings.TilerSettings.aspectRatioLowerThr
</code></pre>

<p><strong>Description:</strong> A float attribute that raises the threshold for tiling boxes with smaller aspect ratios. Defines the lower limit - only boxes wider than this value get tiled (controls which boxes are considered "elongated"), since they likely contain long text strings. Tune this parameter together with <code>aspectRatioUpperThr</code>.</p>

<p><strong>Tuning effect:</strong> Decreasing this threshold results in more rectangular-shaped (low-aspect ratio) boxes being tiled. If the desired text box is not tiled, decreasing this parameter may help. Increase this threshold to tile only very long boxes.</p>

<p><strong>Default:</strong> 10.0f</p>

<p><strong>Valid range:</strong> [1.0f, inf]</p>

<p><strong>Return Value:</strong> Float value representing the lower threshold for aspect ratios.</p>

<hr>

<h4 id="aspectratioupperthr" class="anchor"><a class="heading-anchor" href="#aspectratioupperthr"><span></span></a>aspectRatioUpperThr</h4>

<pre class="prettyprint"><code>    float TextOCR.Settings.TilerSettings.aspectRatioUpperThr
</code></pre>

<p><strong>Description:</strong> Defines the upper limit – only boxes up to this width/height ratio get tiled (prevents superlong, odd boxes being tiled). Filters boxes with very high aspect ratios, may rarely occur and be false positives from the text detector model. Tune this parameter together with <code>aspectRatioLowerThr</code>. A similar effect can be achieved with the <code>minBoxSize</code> parameter.</p>

<p><strong>Tuning effect:</strong> Increasing this parameter allows tiling of more long and narrow boxes. Decreasing this parameter avoids tiling extremely stretched or odd-shaped boxes.</p>

<p><strong>Default:</strong> 40.0f</p>

<p><strong>Valid range:</strong> [1.0f, inf]</p>

<p><strong>Return Value:</strong> Float value representing the upper threshold for aspect ratios.</p>

<hr>

<h4 id="topkmergedpredictions" class="anchor"><a class="heading-anchor" href="#topkmergedpredictions"><span></span></a>topkMergedPredictions</h4>

<pre class="prettyprint"><code>Integer TextOCR.Settings.TilerSettings.topkMergedPredictions
</code></pre>

<p><strong>Description:</strong> Limits the number of decodes returned based on confidence scores. This affects how many merged combinations are returned during the tiling stage.</p>

<p><strong>Tuning effect:</strong> Increasing this parameter increases the number of possible results to review. Decreasing this parameter results to fewer, faster results.</p>

<p><strong>Default:</strong> 5</p>

<p><strong>Valid range:</strong> [1, max(int)]</p>

<p><strong>Return Value:</strong> Integer representing the top merged predictions to return.</p>

<hr>

<h4 id="advancedtilingparameters" class="anchor"><a class="heading-anchor" href="#advancedtilingparameters"><span></span></a>Advanced Tiling Parameters</h4>

<p>These advanced tiling parameters are only intended to be utilized in edge cases that are difficult to solve. Zebra recommends not to change these parameters unless necessary.</p>

<hr>

<h5 id="topcorrelationthr" class="anchor"><a class="heading-anchor" href="#topcorrelationthr"><span></span></a>topCorrelationThr</h5>

<pre class="prettyprint"><code>    Float TextOCR.Settings.TilerSettings.topCorrelationThr
</code></pre>

<p><strong>Description:</strong> Sets the threshold for correlation to consider merging boxes. Increasing this value decreases the number of merge points considered.</p>

<p><strong>Tuning effect:</strong> Increasing this value restricts the internal merging mechanism to use only points with a correlation score higher than this value. Setting it to 0 removes the limit. If incorrect tiling occurs, increasing this parameter may help.</p>

<p><strong>Default:</strong> 0.0f</p>

<p><strong>Valid range:</strong> <code>[0.0f, 1.0f]</code></p>

<p><strong>Return Value:</strong> Float value representing the correlation threshold value.</p>

<hr>

<h5 id="mergepointscutoff" class="anchor"><a class="heading-anchor" href="#mergepointscutoff"><span></span></a>mergePointsCutoff</h5>

<pre class="prettyprint"><code>    Integer TextOCR.Settings.TilerSettings.mergePointsCutoff
</code></pre>

<p><strong>Description:</strong> Determines the cutoff for the number of merge points. If the number exceeds this value, merging is not performed. This internal parameter limits the number of possible combinations used for tile merging. </p>

<p><strong>Tuning effect:</strong> Increasing this value results in more combinations being used, increasing processing time but potentially generating more accurate results.</p>

<p><strong>Default:</strong> 5</p>

<p><strong>Valid range:</strong> [1, max(int)]</p>

<p><strong>Return Value:</strong> Integer representing the maximum number of merge points allowed.</p>

<hr>

<h5 id="splitmarginfactor" class="anchor"><a class="heading-anchor" href="#splitmarginfactor"><span></span></a>splitMarginFactor</h5>

<pre class="prettyprint"><code>    Float TextOCR.Settings.TilerSettings.splitMarginFactor
</code></pre>

<p><strong>Description:</strong> Reduces the probability of characters appearing at the end due to splitting. </p>

<p><strong>Default</strong>: 0.1f</p>

<p><strong>Valid range</strong>: <code>[0.0f, 1.0f]</code></p>

<p><strong>Return Value:</strong> Float value representing the factor applied to margin splitting.</p>

<hr>

<h4 id="samplecode-3" class="anchor"><a class="heading-anchor" href="#samplecode-3"><span></span></a>Sample Code</h4>

<p>The <code>TilerSettings</code> object is part of the <code>TextOCR.Settings</code> configuration. Access and modify <code>TilerSettings</code> through the <code>TextOCR.Settings</code> object.</p>

<p>This sample code demonstrates how to configure <code>TilerSettings</code> and process the image for text detection and recognition:</p>

<ol>
<li><p><strong>Initialize Settings:</strong> Begin by creating a <code>TextOCR.Settings</code> instance.</p></li>

<li><p><strong>Configure TilerSettings:</strong> Access the <code>TilerSettings</code> within the <code>TextOCR.Settings</code> instance and set custom values for tiling parameters to control how the image is divided and processed.</p></li>

<li><p><strong>Instantiate TextOCR:</strong> Use the configured settings to create a <code>TextOCR</code> instance. This object will handle the text detection and recognition processes.</p></li>

<li><p><strong>Load Bitmap Image:</strong> Prepare the image for OCR by converting it to a <code>Bitmap</code> object.</p></li>

<li><p><strong>Perform Detection:</strong> Use the detect method to analyze the image and retrieve an array of <a href="../types/#ocrresult">OCRResult</a> objects containing the detected text.</p></li>

<li><p><strong>Print Results:</strong> Iterate over the <code>OCRResult</code> array to output the recognized text to the console.</p></li>

<li><p><strong>Dispose Resources:</strong> Free up system resources by calling the dispose method on the TextOCR object after usage.</p>

<pre class="prettyprint"><code>import com.zebra.aivision.TextOCR;
import com.zebra.aivision.TextOCR.Settings;
import com.zebra.aivision.TextOCR.Settings.TilerSettings;
import android.graphics.Bitmap;

// Initialize settings with custom tiling options
TextOCR.Settings.TilerSettings tilerSettings = new TextOCR.Settings.TilerSettings();
tilerSettings.tiling.enable = true;
tilerSettings.tiling.aspectRatioLowerThr = 8.0f;
tilerSettings.tiling.aspectRatioUpperThr = 35.0f;
tilerSettings.tiling.mergePointsCutoff = 10;

// Initialize executor
Executor executor = Executors.newFixedThreadPool(1);

// Input params: bitmap image (to perform detection) and an executor thread object (in which the detection happens and the results are returned)
CompletableFuture&lt;OCRResult[]&gt; futureResult = textocr.detect(bitmap,executor);

futureResult.thenAccept (ocrResults -&gt; {
    //Process the returned output that contains complex bounding boxes and text in it.
});

// Dispose resources
// Once done using the textOCR object, dispose it to release resources and memory used for detection.
textOCR.dispose()
</code></pre></li>
</ol>

<hr>

<h2 id="textgrouping" class="anchor"><a class="heading-anchor" href="#textgrouping"><span></span></a>Text Grouping</h2>

<p>After Words are identified and decoded from the Text Recognition stage, the Text Grouping stage organizes them into lines or paragraphs. This process is carried out in 2 steps: </p>

<ol>
<li><a href="../types/#word">Words</a> detected by OCR are grouped into <a href="../types/#textline">Lines</a>.</li>

<li>The Lines are further grouped into <a href="../types/#textparagraph">Paragraphs</a>. </li>
</ol>

<p>In the graphic representation below, Words, Lines and Paragraphs are represented by blue, green and fuchsia borders, respectively.</p>

<p><img alt="image" style="height:325px" src="./schema.png"></p>

<hr>

<h3 id="groupersettings" class="anchor"><a class="heading-anchor" href="#groupersettings"><span></span></a>Grouper Settings</h3>

<p>The <code>GrouperSettings</code> class provides parameters for customizing the behavior of the OCR text grouping algorithm. It offers control over how text elements are spatially organized based on their geometric properties. By adjusting these settings, developers can fine-tune how text boxes are grouped into lines, paragraphs, or other structures based on their spatial relationships.</p>

<hr>

<h4 id="widthdistanceratio" class="anchor"><a class="heading-anchor" href="#widthdistanceratio"><span></span></a>widthDistanceRatio</h4>

<pre class="prettyprint"><code>    Float TextOCR.Settings.GrouperSettings.widthDistanceRatio
</code></pre>

<p><strong>Description:</strong> Determines the threshold for joining Words into Lines. Adjusting this parameter allows control over acceptable spacing between Words in a Line. Words spaced beyond this threshold are treated as separate Lines. The default value of 1.5f indicates that the acceptable space between Words should not exceed 50% of their average width. Increasing this value to 2.0f allows for a maximum acceptable space of 100% of the average Word width.</p>

<p>For example, if the average Word width is 90 pixels, widthDistanceRatio of 2.0 allows words with centers up to 180 pixels apart to be grouped into the same Line.</p>

<p><strong>Tuning effect:</strong> Increasing this parameter causes horizontally spaced Words to join into a Line. Set this value higher if Words are spaced further apart and should be joined into a Line, such as in artistic layouts.</p>

<p><strong>Default:</strong> 1.5f</p>

<p><strong>Valid range:</strong> [0.0f, inf]</p>

<p><img alt="image" style="height:100px" src="./widthDistanceRatio.png"></p>

<hr>

<h4 id="heightdistanceratio" class="anchor"><a class="heading-anchor" href="#heightdistanceratio"><span></span></a>heightDistanceRatio</h4>

<pre class="prettyprint"><code>    Float TextOCR.Settings.GrouperSettings.heightDistanceRatio
</code></pre>

<p><strong>Description:</strong> Affects the grouping of Words into Lines, particularly in scenarios where text undergoes a sudden change in font size but should still be grouped together. Although the algorithm has no knowledge of the actual font size, it uses the height of the complex bounding box to approximate it. The default value of 2.0f indicates that Words will be grouped together even if their font size differs by up to twice the height.</p>

<p>For example, setting this parameter to 4.0 allows words with height differences up to 4 times to be grouped into the same Line.</p>

<p><strong>Tuning effect:</strong> Increasing this parameter allows words of varying heights to join into a single Line. Raise this value higher when there is significant variation in text sizes within the same line, such as in documents with mixed fonts. Decrease this parameter if strange font-size jumps are creating messy lines.</p>

<p><strong>Default</strong>: 2.0f</p>

<p><strong>Valid range:</strong> [1.0f, inf]</p>

<p><img alt="image" style="height:100px" src="./heightDistanceRatio.jpg"></p>

<hr>

<h4 id="centerdistanceratio" class="anchor"><a class="heading-anchor" href="#centerdistanceratio"><span></span></a>centerDistanceRatio</h4>

<pre class="prettyprint"><code>Float TextOCR.Settings.GrouperSettings.centerDistanceRatio
</code></pre>

<p><strong>Description:</strong> Affects the joining of Words into Lines, particularly in scenarios where lines of text are not perfectly straight, such as in curved lines of text. The threshold value should be adjusted empirically, as it mathematically represents the relationship between the positions of two consecutive Words.</p>

<p>For example, if the average Word height is 20 pixels, setting centerDistanceRatio to 1.0 allows Words with centers up to 20 pixels apart vertically to be grouped into the same Line.</p>

<p><strong>Tuning effect:</strong> Increasing this parameter allows Words that are not vertically aligned to be joined into the same Line. Decrease this value if only straight lines should be grouped.</p>

<p><strong>Default:</strong> 0.6f</p>

<p><img alt="image" style="height:100px" src="./centerDistanceRatio.png"></p>

<hr>

<h4 id="paragraphheightdistance" class="anchor"><a class="heading-anchor" href="#paragraphheightdistance"><span></span></a>paragraphHeightDistance</h4>

<pre class="prettyprint"><code>    Float TextOCR.Settings.GrouperSettings.paragraphHeightDistance
</code></pre>

<p><strong>Description:</strong> Determines the difference in vertical spacing between the center of two Lines to determine if they should be grouped into a Paragraph. It is particularly useful when the Lines of text have unusually large "leading", which refers to the distance between consecutive Lines in a Paragraph. The default value of 1.0f indicates that the Lines can be grouped into a paragraph if their centers are spaced apart by 100% of their average height.</p>

<p>For example, if the average Line height is 30 pixels, setting this parameter to 2 allows Lines with centers up to 60 pixels apart to be grouped into a Paragraph.</p>

<p><strong>Tuning effect:</strong> Increasing this parameter allows Lines that are spaced farther apart vertically to be joined into a Paragraph. Consider raising this value higher for documents with widely spaced Lines. Decrease this value if too many lines are getting grouped.</p>

<p><strong>Default:</strong> 1.0f</p>

<p><strong>Valid range:</strong> [0.0f, inf]</p>

<p><img alt="image" style="height:150px" src="./paragraphHeightDistance.png"></p>

<hr>

<h4 id="paragraphheightratiothreshold" class="anchor"><a class="heading-anchor" href="#paragraphheightratiothreshold"><span></span></a>paragraphHeightRatioThreshold</h4>

<pre class="prettyprint"><code>    Float TextOCR.Settings.GrouperSettings.paragraphHeightRatioThreshold
</code></pre>

<p><strong>Description:</strong> Determines if there is a significant height difference between two rows, expressed as a ratio of the heights of two adjacent Lines, to decide whether they should be joined into a Paragraph. This can be useful in scenarios when Lines of varying font sizes should be joined into a single Paragraph. Although the algorithm has no knowledge of actual font sizes, it uses the height of the complex bounding box as an approximation. The default value of 1.0/3.0f (approximately 0.33) indicates that if consecutive Lines differ in height by a facotr of up to 3, they will still be grouped together into a single Paragraph.</p>

<p>For example, if the average Line height is 50 pixels, setting this parameter to 0.2 allows Lines with heights ranging from approximately 10 pixels to 250 pixels to be grouped into the same Paragraph.</p>

<p><strong>Tuning effect:</strong> Decreasing this parameter allows Lines with larger height differences to be joined into a Paragraph, which can be useful for documents with diverse fonts. Increase this parameter to only group similar-sized lines.</p>

<p><strong>Default:</strong> 0.33f</p>

<p><strong>Valid range:</strong> [0.0f, 1.0f]</p>

<p><img alt="image" style="height:150px" src="./paragraphHeightRatioThreshold.png"></p>

<hr>

<h4 id="samplecode-4" class="anchor"><a class="heading-anchor" href="#samplecode-4"><span></span></a>Sample Code</h4>

<p>To utilize the OCR capabilities of the <code>TextOCR</code> library, follow these steps to configure settings, prepare your image, and perform text detection:</p>

<ol>
<li><p><strong>Configure Settings:</strong> Initialize a <code>TextOCR.Settings</code> object and customize the <code>GrouperSettings</code> parameters for text grouping.</p></li>

<li><p><strong>Asynchronous Initialization:</strong> Use an <code>Executor</code> to initialize the <code>TextOCR</code> instance asynchronously, allowing for efficient resource management and responsiveness.</p></li>

<li><p><strong>Load Bitmap Image:</strong> Prepare the image for OCR by converting it to a Bitmap object.</p></li>

<li><p><strong>Perform OCR:</strong> Use the <code>detect</code> method to analyze the image, retrieving an array of <a href="../types/#ocrresult">OCRResult</a> objects with complex bounding boxes and recognized text.</p></li>

<li><p><strong>Process OCR Results:</strong> Handle the results by iterating over the <code>OCRResult</code> array, outputting the recognized text or using it for further processing.</p></li>

<li><p><strong>Dispose Resources:</strong> After completing OCR operations, call <code>dispose()</code> to release resources and prevent memory leaks.</p>

<pre class="prettyprint"><code>import com.zebra.ai.vision.TextOCR; 

// Initialize TextOCR settings 
String mavenModelName = "text-ocr-recognizer"; 
textOCRSettings = TextOCR.Settings(mavenModelName) 

// Access the GrouperSettings and set custom values for grouping parameters 
textOCRSettings.grouping.widthDistanceRatio = 1.5f; 
textOCRSettings.grouping.heightDistanceRatio = 2.0f; 
textOCRSettings.grouping.centerDistanceRatio = 0.6f; 
textOCRSettings.grouping.paragraphHeightDistance = 1.0f; 
textOCRSettings.grouping.paragraphHeightRatioThreshold = 0.33f; 

// Initialize executor 
Executor executor = Executors.newFixedThreadPool(1); 

// Input params include the bitmap image (to perform detection on) and an executor thread object (in which the detection happens) 
CompletableFuture&lt;OCRResult[]&gt; futureResult = textocr.detect(bitmap,executor); 

futureResult.thenAccept (ocrResults -&gt; { 
    // Process the returned output that contains complex bounding boxes and text within it 
}); 

// Dispose resources 
// Once done using the textOCR object, dispose it to release resources and memory used for detection. 
textOCR.dispose();
</code></pre></li>
</ol>

<hr>

<h2 id="troubleshootingguide" class="anchor"><a class="heading-anchor" href="#troubleshootingguide"><span></span></a>Troubleshooting Guide</h2>

<p><strong>Note:</strong></p>

<ul>
<li>↑ indicates an <strong>increase</strong> in the value or parameter</li>

<li>↓ indicates a <strong>decrease</strong> in the value or parameter</li>
</ul>

<h3 id="quicktipsfordetection" class="anchor"><a class="heading-anchor" href="#quicktipsfordetection"><span></span></a>Quick Tips for Detection</h3>

<p>If the following issues are encountered, try these adjustments:</p>

<table class="facelift" style="width:60%" border="1" padding="5px">
  <thead>
    <tr bgcolor="#dce8ef">
      <th>Issue</th>
      <th>Suggested Adjustment</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Missing faint/small text</td>
      <td>↓ Heatmap and Box Threshold or ↓ Min Box Size/Area</td>
    </tr>
    <tr>
      <td>Too much junk/noise</td>
      <td>↑ Heatmap and Box Threshold or ↑ Min Box Size/Area</td>
    </tr>
    <tr>
      <td>Boxes are too tight, cutting off letters</td>
      <td>↑ Unclip Ratio</td>
    </tr>
    <tr>
      <td>Boxes overlap too much</td>
      <td>↓ Unclip Ratio</td>
    </tr>
    <tr>
      <td>Weird rotations on lines or rules</td>
      <td>↑ Min Ratio for Rotation</td>
    </tr>
    <tr>
      <td>Elongated or tall font styles</td>
      <td>↑ Min Ratio for Rotation</td>
    </tr>
    <tr>
      <td>Need to detect tilted/angled/curved text</td>
      <td>↓ Min Ratio for Rotation and ↑ Unclip Ratio</td>
    </tr>
  </tbody>
</table>

<h3 id="quicktipsforrecognition" class="anchor"><a class="heading-anchor" href="#quicktipsforrecognition"><span></span></a>Quick Tips for Recognition</h3>

<p>If the following issues are encountered, try these adjustments:</p>

<table class="facelift" style="width:60%" border="1" padding="5px">
  <thead>
    <tr bgcolor="#dce8ef">
      <th>Issue</th>
      <th>Suggested Adjustment</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Too many mistakes or incorrect guesses</td>
      <td>↑ Total Probability Threshold</td>
    </tr>
    <tr>
      <td>Missing letters or "�" characters in output</td>
      <td>↓ Total Probability Threshold and ↑ TopK Ignore Cutoff</td>
    </tr>
    <tr>
      <td>Unclear or handwritten text</td>
      <td>↑ TopK Ignore Cutoff and ↓ Total Probability Threshold</td>
    </tr>
    <tr>
      <td>Too many uncertain or incorrect decodes</td>
      <td>↑ Total Probability Threshold</td>
    </tr>
    <tr>
      <td>Missing faint or ambiguous characters</td>
      <td>↓ Total Probability Threshold and ↑ Max Word Combinations</td>
    </tr>
    <tr>
      <td>Need alternatives for post-processing</td>
      <td>↑ Max Word Combinations</td>
    </tr>
  </tbody>
</table>

<h3 id="quicktipsforgrouping" class="anchor"><a class="heading-anchor" href="#quicktipsforgrouping"><span></span></a>Quick Tips for Grouping</h3>

<p>If the following issues are encountered, try these adjustments:</p>

<table class="facelift" style="width:60%" border="1" padding="5px">
  <thead>
    <tr bgcolor="#dce8ef">
      <th>Issue</th>
      <th>Suggested Adjustment</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Words that should be on one line are not</td>
      <td>↑ Width Distance Ratio</td>
    </tr>
    <tr>
      <td>Lines with different font sizes are not grouping</td>
      <td>↑ Height Distance Ratio or ↓ Paragraph Height Ratio Threshold</td>
    </tr>
    <tr>
      <td>Curved or wavy text splits into separate groups</td>
      <td>↑ Center Distance Ratio</td>
    </tr>
    <tr>
      <td>Lines in a paragraph are not grouping</td>
      <td>↑ Paragraph Height Distance</td>
    </tr>
    <tr>
      <td>Lines of different heights are not grouping</td>
      <td>↓ Paragraph Height Ratio Threshold</td>
    </tr>
  </tbody>
</table>

<h3 id="quicktipsfortilingspecialcases" class="anchor"><a class="heading-anchor" href="#quicktipsfortilingspecialcases"><span></span></a>Quick Tips for Tiling (Special Cases)</h3>

<p>If the following issues are encountered, try these adjustments:</p>

<table class="facelift" style="width:60%" border="1" padding="5px">
  <thead>
    <tr bgcolor="#dce8ef">
      <th>Issue</th>
      <th>Suggested Adjustment</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Long, narrow text is not read correctly</td>
      <td>Enable tiling</td>
    </tr>
    <tr>
      <td>Errors appear at the edges of tiles</td>
      <td>Adjust Split Margin Factor (usually leave at default)</td>
    </tr>
    <tr>
      <td>Tiling merges boxes that should not be merged</td>
      <td>↑ Top Correlation Threshold</td>
    </tr>
    <tr>
      <td>Results are slow and perfect accuracy is not needed</td>
      <td>↓ Merge Points Cutoff and ↓ TopK Merged Predictions</td>
    </tr>
    <tr>
      <td>Results on very long text lines are inaccurate</td>
      <td>↑ Merge Points Cutoff and ↑ Aspect Ratio Upper Threshold</td>
    </tr>
  </tbody>
</table>

<hr>

<h2 id="sampleapps" class="anchor"><a class="heading-anchor" href="#sampleapps"><span></span></a>Sample Apps</h2>

<p>Refer to the following resources:</p>

<ul>
<li><strong>Start building your first product and shelf recognizer application</strong> with the <a href="https://github.com/zebradevs/AISuite_Android_Samples/tree/main/AISuite_QuickStart">QuickStart Sample</a> application source.  </li>

<li><strong>Consult the <a href="https://github.com/zebradevs/AISuite_Android_Samples/tree/main/AISuite_Snippets">Java/Kotlin snippets</a>,</strong> which demonstrate the SDK's capabilities and can be easily integrated into your applications. </li>

<li><strong>Access advanced use case and technology-based demos through the Showcase Application,</strong> including he <a href="https://github.com/zebradevs/AISuite_Android_Samples/tree/main/AISuite_Demos/AIDataCaptureDemo">AI DataCapture demo</a>, which outlines how users can enroll and recognize products in real-time. 


<ul>
<li>For instructions on accessing these demo apps, refer to the <a href="https://www.zebra.com/content/dam/zebra_dam/en/presentation/customer-facing/zebra-mobile-computing-ai-suite-presentation-customer-showcase-ai-data-capture-barcode-finder-demo-application-installation.pdf">installation guide</a> </li>

<li>Access source code for these demos in the <a href="https://github.com/zebradevs/AISuite_Android_Samples/tree/main/AISuite_Demos">ZebraDevs github repo</a>, including the <a href="https://github.com/zebradevs/AISuite_Android_Samples/tree/main/AISuite_Demos/AIDataCaptureDemo">AI Data Capture Demo</a> source, to easily build these capabilities into your application.</li></ul>
</li>
</ul>

<hr>

<h2 id="relatedguides" class="anchor"><a class="heading-anchor" href="#relatedguides"><span></span></a>Related Guides</h2>

<ul>
<li><a href="../about/">About</a></li>

<li><a href="../setup/">Setup</a></li>

<li><a href="../localizer/">Localizer</a>


<ul>
<li>Models: <a href="../model/barcode-localizer/">Barcode</a>, <a href="../model/prod-recognizer/">Product &amp; Shelf</a></li></ul>
</li>

<li><a href="../productrecognition/">Product Recognition</a> - <a href="../model/prod-recognizer/">Model</a>


<ul>
<li><a href="../productrecognition/#featureextractor">Feature Extractor</a></li>

<li><a href="../productrecognition/#featurestorage">Feature Storage</a></li>

<li><a href="../productrecognition/#recognizer">Recognizer</a></li></ul>
</li>

<li><a href="../barcodedecoder/">Barcode Decoder</a></li>

<li><a href="../camerax/">CameraX</a>


<ul>
<li><a href="../camerax/#entitytrackeranalyzer">EntityTrackerAnalyzer</a>    </li>

<li><a href="../camerax/#detectors">Detectors</a></li>

<li><a href="../camerax/#entityviewfinder">EntityViewfinder</a></li></ul>
</li>

<li><a href="../entity/">Entity</a></li>

<li><a href="../types/">Data Types</a></li>
</ul><p></p>
                                        


                                    </div>
                                </div>

                            </div>
                        </div>
                    </div>
                </div><!--/.col-sm-6-->
            </div>
            </section>
        </div>
    </section>    

<div class="modal fade" id="basicModal" tabindex="-1" role="dialog" aria-labelledby="basicModal" aria-hidden="true" data-backdrop="true">
			<div class="modal-dialog">
				<div class="modal-content">
					<div class="modal-header">
						<button type="button" class="close" data-dismiss="modal" aria-hidden="true">X</button>
					</div>
					<div class="modal-body">
						<div id="modalImg">
						
						</div>
					</div>
			</div>
		  </div>
		</div> </div>
         <footer id="footer">
            <div class="container-fluid">
                <div class="row">
                    <div class="col-sm-10">
                        ZEBRA and the stylized Zebra head are trademarks of Zebra Technologies Corp., registered in many jurisdictions worldwide. All other trademarks are the property of their respective owners. ©2025 Zebra Technologies Corp. and/or its affiliates.<br> <a href="https://www.zebra.com/us/en/about-zebra/company-information/legal.html">Legal</a> | <a href="https://www.zebra.com/us/en/about-zebra/company-information/compliance/information-privacy/terms-of-use.html">Terms of Use</a> | <a href="https://www.zebra.com/us/en/about-zebra/company-information/legal/privacy-statement.html">Privacy Policy</a> 
                    </div>
                     <!--
                     <div class="col-sm-2">
                        <ul class="social-icons">
                            <li><a href="http://www.facebook.com/pages/Zebra-Technologies/107703715989073"><i class="fa fa-facebook"></i></a></li>
                            <li><a href="https://twitter.com/ZebraDevs"><i class="fa fa-twitter"></i></a></li>
                            <li><a href="http://www.youtube.com/zebratechnologies/"><i class="fa fa-youtube"></i></a></li>
                            <li><a href="https://www.linkedin.com/groups?home=&gid=3220074&trk=anet_ug_hm&goback=%2Egmr_3220074"><i class="fa fa-linkedin"></i></a></li>
                            <li><a href="https://github.com/developer-zebra"><i class="fa fa-github"></i></a></li>
                        </ul>
                    </div> -->
                 </div>
            </div>
        </footer>
    
    <!--/#footer 
        10/25/18 removed <footer id="footer" class="navbar-fixed-bottom">
    -->
    <script src="/js/bootstrap.min.js"></script>
    <script src="/js/owl.carousel.min.js"></script>
    <script src="/js/mousescroll.js"></script>
    <script src="/js/smoothscroll.js"></script>
    <script src="/js/jquery.prettyPhoto.js"></script>
    <script src="/js/jquery.isotope.min.js"></script>
    <script src="/js/jquery.inview.min.js"></script>
    <script src="/js/wow.min.js"></script>
    <script src="/js/bootstrap-treenav.min.js"></script>
    <script src="/js/prettify.js"></script>
    <script src="/js/zepto.js"></script>
    <script src="/js/jquery.waterfall.js"></script>   
    <script src="/js/main.js"></script>

<script type="text/javascript">
  (function(w,d,t,u,n,s,e){w['SwiftypeObject']=n;w[n]=w[n]||function(){
  (w[n].q=w[n].q||[]).push(arguments);};s=d.createElement(t);
  e=d.getElementsByTagName(t)[0];s.async=1;s.src=u;e.parentNode.insertBefore(s,e);
  })(window,document,'script','//s.swiftypecdn.com/install/v2/st.js','_st');

  _st('install','zRzEsAUhWTTkrdEN2YfA','2.0.0');
</script>
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-72584442-1', 'auto');
  ga('send', 'pageview');

</script> 

<script>
    ChatraID = 'QDvZ76min4DhLW5vJ';
    (function(d, w, c) {
        var n = d.getElementsByTagName('script')[0],
            s = d.createElement('script');
        w[c] = w[c] || function() {
            (w[c].q = w[c].q || []).push(arguments);
        };
        s.async = true;
        s.src = (d.location.protocol === 'https:' ? 'https:': 'http:')
            + '//call.chatra.io/chatra.js';
        n.parentNode.insertBefore(s, n);
    })(document, window, 'Chatra');
</script>         


</body></html>